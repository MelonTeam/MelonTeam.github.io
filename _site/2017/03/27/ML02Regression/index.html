<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>机器学习入门系列02，Regression 回归：案例研究</title>
    <meta name="description" content="  为什么要先进行案例研究？  Regression （回归）  引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）  CSDN博客文章地...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2017/03/27/ML02Regression/">
    <link rel="alternate" type="application/rss+xml" title="MelonTeam" href="http://localhost:4000/feed.xml ">



</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">MelonTeam</a>
        <small>移动终端前沿技术的探索者</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/opensource/">
                        
                            <i class="fa fa-folder-open"></i>开源项目
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>归档
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>分类
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>标签
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/board/">
                        
                            <i class="fa fa-pencil"></i>留言板
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>机器学习入门系列02，Regression 回归：案例研究</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2017-03-27
            </div>
            
            
            
            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#机器学习" title="Category: 机器学习" rel="category">机器学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>
            
            
            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E5%9B%9E%E5%BD%92" title="Tag: 回归" rel="tag">回归</a-->
        <a href="/tag/#回归" title="Tag: 回归" rel="tag">回归</a>&nbsp;
    
        <!--a href="/tag/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" title="Tag: 损失函数" rel="tag">损失函数</a-->
        <a href="/tag/#损失函数" title="Tag: 损失函数" rel="tag">损失函数</a>
    
  

</span>

            </div>
            

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#为什么要先进行案例研究" id="markdown-toc-为什么要先进行案例研究">为什么要先进行案例研究？</a></li>
  <li><a href="#regression-回归" id="markdown-toc-regression-回归">Regression （回归）</a>    <ul>
      <li><a href="#应用举例预测pokemon-go-进化后的战斗力" id="markdown-toc-应用举例预测pokemon-go-进化后的战斗力">应用举例（预测Pokemon Go 进化后的战斗力）</a>        <ul>
          <li><a href="#三个步骤" id="markdown-toc-三个步骤">三个步骤</a></li>
          <li><a href="#step1-model" id="markdown-toc-step1-model">Step1: Model</a></li>
          <li><a href="#step2-方程的好坏" id="markdown-toc-step2-方程的好坏">Step2: 方程的好坏</a></li>
          <li><a href="#step3最好的方程" id="markdown-toc-step3最好的方程">Step3：最好的方程</a></li>
          <li><a href="#gradient-descent梯度下降法" id="markdown-toc-gradient-descent梯度下降法">Gradient Descent（梯度下降法）</a></li>
        </ul>
      </li>
      <li><a href="#结果怎么样呢" id="markdown-toc-结果怎么样呢">结果怎么样呢？</a></li>
      <li><a href="#overfitting过拟合过度学习" id="markdown-toc-overfitting过拟合过度学习">Overfitting（过拟合，过度学习）</a></li>
      <li><a href="#如果数据更多会怎样" id="markdown-toc-如果数据更多会怎样">如果数据更多会怎样？</a></li>
      <li><a href="#还有其他因素的影响吗" id="markdown-toc-还有其他因素的影响吗">还有其他因素的影响吗？</a></li>
      <li><a href="#regularization正则化" id="markdown-toc-regularization正则化">Regularization（正则化）</a></li>
    </ul>
  </li>
  <li><a href="#总结" id="markdown-toc-总结">总结</a></li>
</ul>
<blockquote>
  <p>引用课程：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></p>
</blockquote>

<p>先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）</p>

<blockquote>
  <p>CSDN博客文章地址：<a href="http://blog.csdn.net/zyq522376829/article/details/66577532">http://blog.csdn.net/zyq522376829/article/details/66577532</a></p>
</blockquote>

<h1 id="为什么要先进行案例研究">为什么要先进行案例研究？</h1>

<p>没有比较好的数学基础，直接接触深度学习会非常抽象，所以这里我们先通过一个预测 Pokemon Go 的 Combat Power (CP) 值的案例，打开深度学习的大门。</p>

<h1 id="regression-回归">Regression （回归）</h1>

<!--more-->

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0me8ssa8j20zc0kcn5j.jpg" alt="" /></p>

<h2 id="应用举例预测pokemon-go-进化后的战斗力">应用举例（预测Pokemon Go 进化后的战斗力）</h2>

<p>比如估计一只神奇宝贝进化后的 CP 值（战斗力）。
下面是一只妙蛙种子，可以进化为妙蛙草，现在的CP值是14，我们想估计进化后的CP值是多少；进化需要糖果，好处就是如果它进化后CP值不满意，那就不用浪费糖果来进化它了，可以选择性价比高的神奇宝贝。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mf7rxiij20xo0hgn53.jpg" alt="" /></p>

<p>输入用了一些不同的 $x$ 来代表不同的属性，比如战斗力用 $x_{cp}$ 来表示，物种 $x_{s}$ 来表示…
输出就是进化后的CP值</p>

<h3 id="三个步骤">三个步骤</h3>
<p>上一篇提到了机器学习的三个步骤：
Step1.确定一组函数（Model）。
Step2.将训练集对函数集进行训练。
Step3.挑选出“最好”的函数 $f^{<em>}$ 
然后就可以使用 $f^{</em>}$ 来对新的测试集进行检测。</p>

<h3 id="step1-model">Step1: Model</h3>
<p>这个model 应该长什么样子呢，先写一个简单的：我们可以认为进化后的CP值 $y$ 等于进化前的CP值 $x_{cp}$ 乘以一个参数 $w$ 再加上一个参数 $b$ 。</p>

<script type="math/tex; mode=display">y = b + w \cdot x_{cp}   \qquad (1-1)</script>

<p>$w$ 和 $b$ 是参数，可以是任何数值。</p>

<p>可以有
<script type="math/tex">f_{1}: y = 10.0 + 9.0 \cdot x_{cp}</script></p>

<script type="math/tex; mode=display">f_{2}: y = 9.8 + 9.2 \cdot x_{cp}</script>

<script type="math/tex; mode=display">f_{3}: y = -0.8 -1.2 \cdot x_{cp}</script>

<p>这个函数集中可以有无限多的 function。所以我们用 $y = b + w \cdot x_{cp} $ 代表这些 function 所成的集合。还有比如上面的 $f_{3}$ ，明显是不正确的，因为CP值有个条件都是正的，那乘以 $-1.2$ 就变成负的了，所以我们接着就要根据训练集来找到，这个 function set 里面，哪个是合理的 function。</p>

<p>我们将式1-1 称作 Linear model， Linear model 形式为：</p>

<script type="math/tex; mode=display">y = b + \sum w_{i}x_{i}</script>

<p>$x_{i}$ 就是神奇宝贝的各种不同的属性，身高、体重等等，我们将这些称之为 “feature（特征）”；$w_{i}$ 称为 weight（权重），b 称为 bias（偏差）。</p>

<h3 id="step2-方程的好坏">Step2: 方程的好坏</h3>
<p>现在就需要搜集训练集，这里的数据集是 Supervised 的，所以需要 function 的输入和输出（数值），举例抓了一只杰尼龟，进化前的CP值为612，用 $x^{1}$ 代表这只杰尼龟进化前的CP值，即用上标标示一个完整对象的编号；进化后的CP值为 979，用 $\hat{y}^{1}$ 表示进化后的CP值，用 hat（字母头顶的上尖符号）来表示这是一个正确的值，是实际观察到function该有的输出。</p>

<p>下面我们来看真正的数据集（来源 Source: <a href="https://www.openintro.org/stat/data/?data=pokemon">https://www.openintro.org/stat/data/?data=pokemon</a>）</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mfqcewwj211i0jiwhn.jpg" alt="" /></p>

<p>来看10只神奇宝贝的真实数据，$x$ 轴代表进化前的CP值，$y$ 轴代表进化后的CP值。</p>

<p>有了训练集，为了评价 function 的好坏，我们需要定义一个新的函数，称为 <strong>Loss function (损失函数)</strong>，定义如下：</p>

<p>Loss function $L$ :</p>

<p>input: a function, output: how bad it is</p>

<p>Loss function是比较特别的函数，是函数的函数，因为它的输入是一个函数，而输出是表示输入的函数有多不好。 可以写成下面这种形式：</p>

<script type="math/tex; mode=display">L(f) = L(w, b)</script>

<p>损失函数是由一组参数 w和b决定的，所以可以说损失函数是在衡量一组参数的好坏。</p>

<p>这里用比较常见的定义形式：</p>

<script type="math/tex; mode=display">L(f) = L(w, b) =\sum_{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x_{cp}^{n})\right)^{2} \qquad   (1-2)</script>

<p>将实际的数值 $\hat{y}^{n}$ 减去 估测的数值 $b + w\cdot x_{cp}^{n}$，然后再给平方，就是 Estimation error（估测误差，总偏差）；最后将估测误差加起来就是我们定义的损失函数。</p>

<p>这里不取各个偏差的代数和$\sum_{n=1}^{10}\hat{y}^{n} -(b + w\cdot x_{cp}^{n})$ 作为总偏差，这是因为这些偏差（$\hat{y}^{i} -(b + w\cdot x_{cp}^{i})$）本身有正有负，如果简单地取它们的代数和，就可能互相抵消，这是虽然偏差的代数和很小，却不能保证各个偏差都很小。所以按照式1-2，是这些偏差的平方和最小，就可以保证每一个偏差都很小。</p>

<p>为了更加直观，来对损失函数进行作图：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mg9bqyyj210o0o678o.jpg" alt="" /></p>

<p>图上每个点都代表一个方程，比如红色的那个点代表 $y=-180-2\cdot x_{cp}$ 。颜色代表用这个点的方程得到的损失函数有多不好，颜色越偏红色，代表数值越大，越偏蓝色蓝色，代表方程越好。最好的方程就是图中叉叉标记的点。</p>

<h3 id="step3最好的方程">Step3：最好的方程</h3>

<p>定好了损失函数，可以衡量每一个方程的好坏，接下来需要从函数集中挑选一个最好的方程。将这个过程数学化：</p>

<script type="math/tex; mode=display">f^{*}=\arg \min_{f} L(f)</script>

<script type="math/tex; mode=display">w^{*},b^{*} = \arg \min_{w,b} L(w,b)=\arg \min_{w,b} \sum_{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x_{cp}^{n})\right)^{2} \qquad  (1-3)</script>

<p>由于这里举例的特殊性，对于式1-3，直接使用最小二乘法即可解出最优的 w 和 b，使得总偏差最小。</p>

<blockquote>
  <p>简单说一下<strong>最小二乘法</strong>，对于二元函数 $f(x,y)$，函数的极值点必为 $\frac{\partial f}{\partial x}$ 及$\frac{\partial f}{\partial y}$ 同时为零或至少有一个偏导数不存在的点；这是极值的必要条件。用这个极值条件可以解出w 和 b。（详情请参阅《数学分析，第三版下册，欧阳光中 等编》第十五章，第一节）</p>
</blockquote>

<p>但这里会使用另外一种做法，<strong>Gradient Descent（最速下降法）</strong>，最速下降法不光能解决式1-3 这一种问题；实际上只要 $L$ 是可微分的，都可以用最速下降法来处理。</p>

<h3 id="gradient-descent梯度下降法">Gradient Descent（梯度下降法）</h3>
<p>简单来看一下梯度下降法的做法。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mgsxi8wj212i0ocjwj.jpg" alt="" /></p>

<p>考虑只有一个参数 $w$ 的损失函数，随机的选取一个初始点，计算 $w = w^{0}$ 时 $L$ 对 $w$ 的微分，然后顺着切线下降的方向更改 $w$ 的值（因为这里是求极小值），即斜率为负，增加$w$ ；斜率为正，减小$w$ .</p>

<table>
  <tbody>
    <tr>
      <td>那么每次更改 $w$ ，更改多大，用 $\eta \frac{\mathrm{d}L}{\mathrm{d}w}</td>
      <td>_{w=w^{0}}$ 表示，$\eta$ 被称为“learning rate”学习速率。</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>由于这里斜率是负的，所以是 $w^{0} - \eta \frac{\mathrm{d}L}{\mathrm{d}w}</td>
      <td>_{w=w^{0}}$ ，得到 $w^{1}$；接着就是重复上述步骤。</td>
    </tr>
  </tbody>
</table>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mh9cr4mj21220najwa.jpg" alt="" /></p>

<p>直到找到一个点，这个点的斜率为0。但是例子中的情况会比较疑惑，这样的方法很可能找到的只是局部极值，并不是全局极值，但这是由于我们例子的原因，针对回归问题来说，是不存在局部极值的，只有全局极值。所以这个方法还是可以使用。</p>

<p>下面来看看两个参数的问题。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mhq93m2j210c0lg0w5.jpg" alt="" /></p>

<p>两个参数的区别就是每次需要对两个参数求偏微分，然后同理更新参数的值。</p>

<blockquote>
  <p>关于梯度可以参阅《数学分析，第三版下册，欧阳光中 等编》，第十四章第六节。也可以大概看看<a href="http://baike.baidu.com/link?url=zcPz3cjP_TPVZOHaLK49CD8CBSRJgD2ZyJ8fq9FOlqB6zX1NHw5p5ilA-CDTc87ujD3waWQygEnhPL7ERofrbI7AhRWLMDKTCtLXjbhFvve">百度百科</a>又或者<a href="https://en.wikipedia.org/wiki/Gradient">wikipedia</a></p>
</blockquote>

<p>将上述做法可视化：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mi9ztuvj20y80maqnz.jpg" alt="" /></p>

<p>同理梯度下降的缺陷如下图：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mir5gsdj212c0lo7qj.jpg" alt="" /></p>

<p>可能只是找到了局部极值，但是对于线性回归，可以保证所选取的损失函数式1-2是 convex（凸的，即只存在唯一极值）。上图右边就是损失函数的等高线图，可以看出是一圈一圈向内减小的。</p>

<h2 id="结果怎么样呢">结果怎么样呢？</h2>

<p>将求出的结果绘图如下</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mmwc0tgj212m0l4tck.jpg" alt="" /></p>

<p>可以计算出训练集上的偏差绝对值之和为 31.9</p>

<p>但真正关心的并不是在训练集上的偏差，而是Generalization的情况，就是需要在新的数据集（测试集）上来计算偏差。如下图：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnf5r0uj212o0lmn22.jpg" alt="" /></p>

<p>使用十个新的神奇宝贝的数据作为测试集计算出偏差绝对值之和为35.</p>

<p>接下来考虑是否能够做的更好，可能并不只是简单的直线，考虑其他model的情况：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnwbr3gj21360tgn32.jpg" alt="" /></p>

<p>比如重新设计一个model，多一个二次项，来求出参数，得到Average Error为15.4，在训练集上看起来更好了。在测试集上得出的Average Error是18.4，确实是更好的Model。</p>

<p>再考虑三次项：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0moh17mgj213g0tsgs2.jpg" alt="" /></p>

<p>得到的结果看起来和二次项时候的结果差别不大，稍微好一点点。也可以看到$w_{3}$已经非常小了，说明三次项影响已经不大了。</p>

<p>再考虑四次项：</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0moxjsx4j21380to44a.jpg" alt="" /></p>

<p>此时在训练集上可以做的更好，但是测试集的结果变差了。</p>

<p>再考虑五次项：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpebvrmj213c0twjx7.jpg" alt="" /></p>

<p>可以看到测试集的结果非常差。</p>

<h2 id="overfitting过拟合过度学习">Overfitting（过拟合，过度学习）</h2>

<p>将训练集上的Average Error变化进行作图：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpwdlv6j212y0soafr.jpg" alt="" /></p>

<p>可以看到训练集上的 Average Error 逐渐变小。</p>

<p>上面的那些model，高次项是包含低次项的function。理论上确实次幂越高越复杂的方程，可以让训练集的结果越低。但加上测试集的结果：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqdlyzij212o0m6adw.jpg" alt="" /></p>

<p>观察得出结果：虽然越复杂的model可以在训练集上得到更好的结果，但越复杂的model并不一定在测试集上有好的结果。这个结论叫做“<strong>Overfitting（过拟合）</strong>”。</p>

<p>如果此时要选model的话，最好的选择就是三次项式子的model。</p>

<blockquote>
  <p>实际生活中典型的学驾照，学驾照的时候在驾校的训练集上人们可以做的很好，但上路之后真正的测试集就完全无法驾驭。这里只是举个训练集很好，而测试集结果很差的例子^_^</p>
</blockquote>

<h2 id="如果数据更多会怎样">如果数据更多会怎样？</h2>

<p>考虑60只神奇宝贝的数据</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqsw1iuj20y40ms7ff.jpg" alt="" /></p>

<p>可以看出物种也是一个关键性的因素，只考虑进化前的CP值是太局限的，刚才的model就设计的不太好。</p>

<p>新的model如下</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mr9qf2mj20yo0joq51.jpg" alt="" /></p>

<p>将这个model写成linear model的形式：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mrpzq6hj21bm0u0qof.jpg" alt="" /></p>

<p>来看做出来的结果：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0ms6u00hj21020t6gs0.jpg" alt="" /></p>

<p>不同种类的神奇宝贝用的参数不同，用颜色区分。此时model在训练集上可以做的更好，在测试集上的结果也是比之前的18.1更好。</p>

<h2 id="还有其他因素的影响吗">还有其他因素的影响吗？</h2>

<p>比如对身高，体重，生命值进行绘图：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mspg7tjj213k0sgdkf.jpg" alt="" /></p>

<p>重新设计model：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mt6esluj213c0m6788.jpg" alt="" /></p>

<p>考虑上生命值（$x_{hp}$）、高度（$x_{h}$）、重量（$x_{w}$）</p>

<p>这么复杂的model，理论上训练集上可以得到更好的结果，实际为1.9，确实是更低。但是测试集的结果就过拟合了。</p>

<h2 id="regularization正则化">Regularization（正则化）</h2>

<p>对于上面那么多参数结果并不理想的情况，这里进行正则化处理，将之前的损失函数进行修改：</p>

<script type="math/tex; mode=display">y = b + \sum w_{i}x_{i}  \qquad  (1-4)</script>

<script type="math/tex; mode=display">L(f) = L(w, b) =\sum_{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x_{cp}^{n})\right)^{2} + \lambda \sum (w_{i})^{2} \qquad   (1-5)</script>

<p>式1-5 中多加了一项： $\lambda \sum (w_{i})^{2}$ ，结论是$w_{i}$越小，则方程（式1-4）就越好。还可以说当 $w_{i}$ 越小，则方程越平滑。</p>

<p>平滑的意思是当输入变化时，输出对输入的变化不敏感。比如式1-5 中输入增加了 $\Delta x_{i}$ 则输入就增加了 $w_{i}\Delta x_{i}$ ，可以看出当$w_{i}$越小，输出变化越不明显。还比如测试集的输入有一些噪音数据，越平滑的方程就会受到更小的影响。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mtp38ryj212g0smte8.jpg" alt="" /></p>

<p>上图是对 $\lambda$进行调整得出的结果。当 $\lambda$ 越大的时候， $\lambda \sum (w_{i})^{2}$ 这一项的影响力越大，所以当$\lambda$  越大的时候，方程越平滑。</p>

<p>训练集上得到的结果是：当 $\lambda$ 越大的时候，在训练集上得到的Error 是越大的。这是合理的现象，因为当 $\lambda$ 越大的时候，就越倾向于考虑 $w$ 本身值，减少考虑error。但是测试集上得到的error 是先减小又增大的。这里喜欢比较平滑的function，因为上面讲到对于噪音数据有很好的鲁棒性，所以开始增加 $\lambda$ 的时候性能是越来越好；但是又不喜欢太平滑的function，最平滑的function就是一条水平线了，那就相当于什么都没有做，所以太平滑的function又会得到糟糕的结果。</p>

<p>所以最后这件事情就是找到最合适的 $\lambda$ ，此时带进式1-5 求出$b$ 和 $w_{i}$，得到的function就是最优的function。</p>

<blockquote>
  <p>对于Regularization 的时候，多加的一项：$\lambda \sum (w_{i})^{2}$，并没有考虑 $b$ ，是因为期望得到平滑的function，但bias这项并不影响平滑程度，它只是将function上下移动，跟function的平滑程度是没有关系的。</p>
</blockquote>

<h1 id="总结">总结</h1>

<ul>
  <li><strong>Pokemon</strong>：原始的CP值极大程度的决定了进化后的CP值，但可能还有其他的一些因素。</li>
  <li><strong>Gradient descent</strong>：梯度下降的做法；后面会讲到它的理论依据和要点。</li>
  <li><strong>Overfitting</strong>和<strong>Regularization</strong>：过拟合和正则化，主要介绍了表象；后面会讲到更多这方面的理论</li>
</ul>

<blockquote>
  <p>新博客地址：<a href="http://yoferzhang.com/post/20170326ML02Regression">http://yoferzhang.com/post/20170326ML02Regression</a></p>
</blockquote>


        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2017/03/27/ML01Introduction/">机器学习入门系列01，Introduction 简介</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2017/03/27/ML03BiasAndVariance/">机器学习入门系列03，Error的来源：偏差和方差（bias 和 variance）</a></p>
        
    </div>
</div>


        <h2 id="comments">说一说</h2>
        

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript">
var uyan_config = {
     'url':'http://localhost:4000/2017/03/27/ML02Regression/',
     'du':'http://localhost:4000', 
     'su':'http://localhost:4000/2017/03/27/ML02Regression/' 
};
</script>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2136610"></script>
<!-- UY END -->






    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    目录
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">说一说</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
            
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/MelonTeam" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
            
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>


<script type="text/javascript" src="http://tajs.qq.com/stats?sId=62569168" charset="UTF-8"></script>


    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
