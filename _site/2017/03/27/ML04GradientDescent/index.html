<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>机器学习入门系列04，Gradient Descent（梯度下降法）</title>
    <meta name="description" content="  什么是Gradient Descent（梯度下降法）？          Review: 梯度下降法        引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的C...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2017/03/27/ML04GradientDescent/">
    <link rel="alternate" type="application/rss+xml" title="MelonTeam" href="http://localhost:4000/feed.xml ">



</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">MelonTeam</a>
        <small>移动终端前沿技术的探索者</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/opensource/">
                        
                            <i class="fa fa-folder-open"></i>开源项目
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>归档
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>分类
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>标签
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/board/">
                        
                            <i class="fa fa-pencil"></i>留言板
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>机器学习入门系列04，Gradient Descent（梯度下降法）</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2017-03-27
            </div>
            
            
            
            <div class="label-card">
            




<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#机器学习" title="Category: 机器学习" rel="category">机器学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>



            </div>
            
            
            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="Tag: 机器学习" rel="tag">机器学习</a-->
        <a href="/tag/#机器学习" title="Tag: 机器学习" rel="tag">机器学习</a>&nbsp;
    
        <!--a href="/tag/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0" title="Tag: 机器学习入门笔记" rel="tag">机器学习入门笔记</a-->
        <a href="/tag/#机器学习入门笔记" title="Tag: 机器学习入门笔记" rel="tag">机器学习入门笔记</a>
    
  

</span>



            </div>
            

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#什么是gradient-descent梯度下降法" id="markdown-toc-什么是gradient-descent梯度下降法">什么是Gradient Descent（梯度下降法）？</a>    <ul>
      <li><a href="#review-梯度下降法" id="markdown-toc-review-梯度下降法">Review: 梯度下降法</a></li>
    </ul>
  </li>
  <li><a href="#tip1调整-learning-rates学习速率" id="markdown-toc-tip1调整-learning-rates学习速率">Tip1：调整 learning rates（学习速率）</a>    <ul>
      <li><a href="#小心翼翼地调整-learning-rate" id="markdown-toc-小心翼翼地调整-learning-rate">小心翼翼地调整 learning rate</a></li>
      <li><a href="#自适应-learning-rate" id="markdown-toc-自适应-learning-rate">自适应 learning rate</a></li>
      <li><a href="#adagrad-算法" id="markdown-toc-adagrad-算法">Adagrad 算法</a>        <ul>
          <li><a href="#adagrad-是什么" id="markdown-toc-adagrad-是什么">Adagrad 是什么？</a></li>
          <li><a href="#adagrad举例" id="markdown-toc-adagrad举例">Adagrad举例</a></li>
          <li><a href="#adagrad-存在的矛盾" id="markdown-toc-adagrad-存在的矛盾">Adagrad 存在的矛盾？</a></li>
          <li><a href="#多参数下结论不一定成立" id="markdown-toc-多参数下结论不一定成立">多参数下结论不一定成立</a></li>
          <li><a href="#adagrad-进一步的解释" id="markdown-toc-adagrad-进一步的解释">Adagrad 进一步的解释</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#tip2stochastic-gradient-descent随机梯度下降法" id="markdown-toc-tip2stochastic-gradient-descent随机梯度下降法">Tip2：Stochastic Gradient Descent（随机梯度下降法）</a></li>
  <li><a href="#tip3feature-scaling特征缩放" id="markdown-toc-tip3feature-scaling特征缩放">Tip3：Feature Scaling（特征缩放）</a>    <ul>
      <li><a href="#为什么要这样做" id="markdown-toc-为什么要这样做">为什么要这样做？</a></li>
      <li><a href="#怎么做-scaling" id="markdown-toc-怎么做-scaling">怎么做 scaling？</a></li>
    </ul>
  </li>
  <li><a href="#梯度下降的理论基础" id="markdown-toc-梯度下降的理论基础">梯度下降的理论基础</a>    <ul>
      <li><a href="#问题" id="markdown-toc-问题">问题</a></li>
    </ul>
  </li>
  <li><a href="#数学理论" id="markdown-toc-数学理论">数学理论</a>    <ul>
      <li><a href="#taylor-series泰勒展开式" id="markdown-toc-taylor-series泰勒展开式">Taylor Series（泰勒展开式）</a>        <ul>
          <li><a href="#定义" id="markdown-toc-定义">定义</a></li>
          <li><a href="#多变量泰勒展开式" id="markdown-toc-多变量泰勒展开式">多变量泰勒展开式</a></li>
        </ul>
      </li>
      <li><a href="#利用泰勒展开式简化" id="markdown-toc-利用泰勒展开式简化">利用泰勒展开式简化</a></li>
    </ul>
  </li>
  <li><a href="#梯度下降的限制" id="markdown-toc-梯度下降的限制">梯度下降的限制</a></li>
</ul>
<blockquote>
  <p>引用课程：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></p>
</blockquote>

<p>先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）</p>

<blockquote>
  <p>CSDN博客文章地址：<a href="http://blog.csdn.net/zyq522376829/article/details/66632699">http://blog.csdn.net/zyq522376829/article/details/66632699</a></p>
</blockquote>

<h1 id="什么是gradient-descent梯度下降法">什么是Gradient Descent（梯度下降法）？</h1>

<p>在第二篇文章中有介绍到梯度下降法的做法，传送门：<a href="地址地址地址">机器学习入门系列02，Regression 回归：案例研究</a></p>

<h2 id="review-梯度下降法">Review: 梯度下降法</h2>

<!--more-->

<p>在回归问题的第三步中，需要解决下面的最优化问题：</p>

<script type="math/tex; mode=display">\theta^{*} = \arg \min_{\theta} L(\theta)</script>

<script type="math/tex; mode=display">L: loss function （损失函数）</script>

<script type="math/tex; mode=display">\theta: parameters （参数）</script>

<blockquote>
  <p>这里的parameters是复数，即 $\theta$ 指代一堆参数，比如上篇说到的 $w$ 和 $b$。</p>
</blockquote>

<p>我们要找一组参数 $\theta$ ，让损失函数越小越好，这个问题可以用梯度下降法解决：</p>

<p>假设 $\theta$ 有里面有两个参数 ${\theta_{1}, \theta_{2}}$</p>

<p>随机选取初始值</p>

<script type="math/tex; mode=display">\theta^{0} = \left[ \begin{matrix} \theta^{0}_{1}\\ \theta^{0}_{2} \end{matrix} \right]</script>

<p>这里可能某个平台不支持矩阵输入，看下图就好。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0p6ppb7dj20z20ck76o.jpg" alt="" /></p>

<p>然后分别计算初始点处，两个参数对 $L$ 的偏微分，然后 $\theta^{0}$ 减掉 $\eta$ 乘上偏微分的值，得到一组新的参数。同理反复进行这样的计算。黄色部分为简洁的写法，$\nabla L(\theta)$即为<strong>梯度</strong>。</p>

<blockquote>
  <p>$\eta$叫做Learning rates（学习速率）</p>
</blockquote>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p768yjij212g0msgps.jpg" alt="" /></p>

<p>上图举例将梯度下降法的计算过程进行可视化。</p>

<h1 id="tip1调整-learning-rates学习速率">Tip1：调整 learning rates（学习速率）</h1>

<h2 id="小心翼翼地调整-learning-rate">小心翼翼地调整 learning rate</h2>

<p>举例：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg" alt="" /></p>

<p>上图左边黑色为损失函数的曲线，假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果  learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果  learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。</p>

<p>虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。</p>

<p>解决方法就是上图右边的方案，将参数改变对损失函数的影响进行可视化。比如 learning rate 太小（蓝色的线），损失函数下降的非常慢；learning rate 太大（绿色的线），损失函数下降很快，但马上就卡住不下降了；learning rate特别大（黄色的线），损失函数就飞出去了；红色的就是差不多刚好，可以得到一个好的结果。</p>

<h2 id="自适应-learning-rate">自适应 learning rate</h2>

<p>举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate</p>

<ul>
  <li>通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate</li>
  <li>update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate</li>
  <li>比如 $\eta^{t} = \eta / \sqrt{t+1}$，$t$ 是次数。随着次数的增加，$\eta^{t}$ 减小</li>
</ul>

<p>但 learning rate 不能是 one-size-fits-all ，不同的参数需要不同的 learning rate</p>

<h2 id="adagrad-算法">Adagrad 算法</h2>

<h3 id="adagrad-是什么">Adagrad 是什么？</h3>

<p>每个参数的学习率都把它除上之前微分的均方根。解释：</p>

<p>普通的梯度下降为：</p>

<script type="math/tex; mode=display">w^{t + 1} \leftarrow w^{t} - \eta^{t} g^{t}</script>

<script type="math/tex; mode=display">\eta^{t} = \frac{\eta}{\sqrt{t+1}}</script>

<p>$w$ 是一个参数</p>

<p>Adagrad 可以做的更好：</p>

<script type="math/tex; mode=display">w^{t+1} \leftarrow w^{t} - \frac{\eta^{t}}{\sigma} g^{t}</script>

<script type="math/tex; mode=display">g^{t} = \frac{\partial L(\theta^{t})}{\partial w}</script>

<p>$\sigma^{t}$:之前参数的所有微分的均方根，对于每个参数都是不一样的。</p>

<h3 id="adagrad举例">Adagrad举例</h3>

<p>下图是一个参数的更新过程</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8iqqphj212a0n241f.jpg" alt="" /></p>

<p>将 Adagrad 的式子进行化简：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8x3ggkj20x00hsac3.jpg" alt="" /></p>

<h3 id="adagrad-存在的矛盾">Adagrad 存在的矛盾？</h3>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9awbcij21080ikmzo.jpg" alt="" /></p>

<p>在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。</p>

<p>下图是一个直观的解释：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9pq9l1j210a0kmmzx.jpg" alt="" /></p>

<p>下面给一个正式的解释：</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pa69l64j212s0ng42l.jpg" alt="" /></p>

<table>
  <tbody>
    <tr>
      <td>比如初始点在 $x_{0}$，最低点为 $-\frac{b}{2a}$，最佳的步伐就是 $x_{0}$ 到最低点之间的距离 $</td>
      <td>x_{0} + \frac{b}{2a}</td>
      <td>$，也可以写成 $\frac{</td>
      <td>2ax_{0} + b</td>
      <td>}{2a}$。而刚好 $</td>
      <td>2ax_{0} + b</td>
      <td>$ 就是方程绝对值在$x_{0}$这一点的微分。</td>
    </tr>
  </tbody>
</table>

<p>这样可以认为如果算出来的微分越大，则距离最低点越远。而且最好的步伐和微分的大小成正比。所以如果踏出去的步伐和微分成正比，它可能是比较好的。</p>

<p>结论1-1：梯度越大，就跟最低点的距离越远。</p>

<p>这个结论在多个参数的时候就不一定成立了。</p>

<h3 id="多参数下结论不一定成立">多参数下结论不一定成立</h3>

<p><strong>对比不同的参数</strong></p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pakwyxsj213i0kik9e.jpg" alt="" /></p>

<p>上图左边是两个参数的损失函数，颜色代表损失函数的值。如果只考虑参数 $w_{1}$，就像图中蓝色的线，得到右边上图结果；如果只考虑参数 $w_{2}$，就像图中绿色的线，得到右边下图的结果。确实对于a和b，结论1-1是成立的，同理c和b也成立。但是如果对比a和c，就不成立了，c比a大，但c距离最低点是比较近的。</p>

<p>所以结论1-1是在没有考虑跨参数对比的情况下，才能成立的。所以还不完善。</p>

<table>
  <tbody>
    <tr>
      <td>之前说到的最佳距离$\frac{</td>
      <td>2ax_{0} + b</td>
      <td>}{2a}$，还有个分母 $2a$ 。对function进行二次微分刚好可以得到：</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">\frac{\partial^{2}y}{\partial x^{2}} = 2a</script>

<p>所以最好的步伐应该是：</p>

<script type="math/tex; mode=display">\frac{一次微分}{二次微分}</script>

<p>即不止和一次微分成正比，还和二次微分成反比。最好的step应该考虑到二次微分：</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pb38dvnj212e0lu4d7.jpg" alt="" /></p>

<h3 id="adagrad-进一步的解释">Adagrad 进一步的解释</h3>

<p>再回到之前的 Adagrad</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pbh8oqtj21200s00xi.jpg" alt="" /></p>

<p>对于$\sqrt{\sum^{t}_{i=0} (g^{i})^{2} }$ 就是希望再尽可能不增加过多运算的情况下模拟二次微分。（如果计算二次微分，在实际情况中可能会增加很多的时间消耗）</p>

<h1 id="tip2stochastic-gradient-descent随机梯度下降法">Tip2：Stochastic Gradient Descent（随机梯度下降法）</h1>

<p>之前的梯度下降：</p>

<script type="math/tex; mode=display">L =\sum_{n} \left( \hat{y}^{n} - (b + \sum w_{i} x^{n}_{i})   \right)^{2}</script>

<script type="math/tex; mode=display">\theta^{i} = \theta^{i -1} - \eta \nabla L(\theta^{i -1})</script>

<p>而Stochastic Gradient Descent（更快）：</p>

<p>损失函数不需要处理训练集所有的数据，选取一个例子 $x^{n}$</p>

<script type="math/tex; mode=display">L^{n} = \left( \hat{y}^{n} - (b + \sum w_{i} x^{n}_{i}   \right)^{2}</script>

<script type="math/tex; mode=display">\theta^{i} = \theta^{i -1} - \eta \nabla L^{n}(\theta^{i -1})</script>

<p>此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数$L^{n}$，就可以赶紧update 梯度。</p>

<p>对比：</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pc5kle1j212g0nq48s.jpg" alt="" /></p>

<p>常规梯度下降法走一步要处理到所有二十个examples，但Stochastic 此时已经走了二十步（没处理一个example就更新）</p>

<h1 id="tip3feature-scaling特征缩放">Tip3：Feature Scaling（特征缩放）</h1>

<p>比如有个function：</p>

<script type="math/tex; mode=display">y = b + w_{1}x_{1} + w_{2}x_{2}</script>

<p>两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pcmc0lcj20zk0iuq8m.jpg" alt="" /></p>

<h2 id="为什么要这样做">为什么要这样做？</h2>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pd1h0xtj213c0m00x5.jpg" alt="" /></p>

<p>上图左边是$x_{1}$的scale比 $x_{2}$要小很多，所以当$w_{1}$ 和 $w_{2}$做同样的变化时，$w_{1}$对y的变化影响是比较小的，$x_{2}$对y的变化影响是比较大的。</p>

<p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为$w_{1}$对y的变化影响比较小，所以$w_{1}$对损失函数的影响比较小，$w_{1}$对损失函数有比较小的微分，所以$w_{1}$方向上是比较平滑的。同理$x_{2}$对y的影响比较大，所以$x_{2}$对损失函数的影响比较大，所以在$x_{2}$方向有比较尖的峡谷。</p>

<p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p>

<p>对于左边的情况，上面讲过这种狭长的情形不过不用Adagrad的话是比较难处理的，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p>

<h2 id="怎么做-scaling">怎么做 scaling？</h2>

<p>方法非常多，这里举例一种常见的做法：</p>

<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdgqdzwj212q0kqn0q.jpg" alt="" /></p>

<p>上图每一列都是一个例子，里面都有一组feature。</p>

<p>对每一个维度$i$（绿色框）都计算平均数，记做$m_{i}$；还要计算标准差，记做$\sigma_{i}$。</p>

<p>然后用第r个例子中的第i个输入，减掉平均数$m_{i}$，然后除以标准差$\sigma_{i}$，得到的结果是所有的维数都是0，所有的方差都是1</p>

<h1 id="梯度下降的理论基础">梯度下降的理论基础</h1>

<h2 id="问题">问题</h2>

<p>当用梯度下降解决问题：</p>

<script type="math/tex; mode=display">\theta^{*} = \arg \min_{\theta} L(\theta)</script>

<p>每次更新参数 $\theta$，都得到一个新的 $\theta$，它都使得损失函数更小。即：</p>

<script type="math/tex; mode=display">L(\theta^{0}) > L(\theta^{1}) > L(\theta^{2})>\cdots</script>

<p>上述结论正确吗？</p>

<p>结论是不正确的。。。</p>

<h1 id="数学理论">数学理论</h1>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdwqnqzj212a0mingb.jpg" alt="" /></p>

<p>比如在$\theta^{0}$处，可以在一个小范围的圆圈内找到损失函数细小的$\theta^{1}$，不断的这样去寻找。</p>

<p>接下来就是如果在小圆圈内快速的找到最小值？</p>

<h2 id="taylor-series泰勒展开式">Taylor Series（泰勒展开式）</h2>

<p>先介绍一下泰勒展开式</p>

<h3 id="定义">定义</h3>

<p>若$h(x)$在$x = x_{0}$点的某个领域内有无限阶导数（即无限可微分，infinitely differentiable），那么在此领域内有：</p>

<script type="math/tex; mode=display">h(x) = \sum^{\infty}_{k = 0} \frac{h^{k}(x_{0})}{k!} (x - x_{0})^{k}</script>

<script type="math/tex; mode=display">=h(x_{0}) + h’(x_{0})(x - x_{0}) + \frac{h’’(x_{0})}{2!}(x - x_{0})^2 + \cdots  \qquad  (1-1)</script>

<p>当$x$很接近$x_{0}$时，有$h(x) \approx h(x_{0}) + h’(x_{0})(x - x_{0})$</p>

<p>式1-1就是函数$h(x)$在$x = x_{0}$点附近关于$x$的幂函数展开式，也叫<strong>泰勒展开式</strong>。</p>

<p>举例：</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0perzpcgj212g0t0qho.jpg" alt="" /></p>

<p>图中3条蓝色线是把前3项作图，橙色线是 $sin(x)$。</p>

<h3 id="多变量泰勒展开式">多变量泰勒展开式</h3>

<p>下面是两个变量的泰勒展开式</p>

<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pf85k6hj212s0jsdic.jpg" alt="" /></p>

<h2 id="利用泰勒展开式简化">利用泰勒展开式简化</h2>

<p>回到之前如何快速在圆圈内找到最小值。基于泰勒展开式，在$(a,b)$ 点的红色圆圈范围内，可以将损失函数用泰勒展开式进行简化：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pfmfsbvj21240j4amj.jpg" alt="" /></p>

<p>将问题进而简化为下图：</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pg12er4j212e0nyk53.jpg" alt="" /></p>

<p>不考虑s的话，可以看出剩下的部分就是两个向量$(\Delta \theta_{1}, \Delta \theta_{2})$ 和 $(u, v)$的内积，那怎样让它最小，就是和向量 $(u, v)$ 方向相反的向量</p>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pghquu2j211y0n4wid.jpg" alt="" /></p>

<p>然后将u和v带入。</p>

<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pgz0bohj213e0nswjd.jpg" alt="" /></p>

<script type="math/tex; mode=display">L(\theta) \approx s + u(\theta_{1} - a) + v(\theta_{2} - b)    (1-2)</script>

<p>发现最后的式子就是梯度下降的式子。但这里用这种方法找到这个式子有个前提，泰勒展开式给的损失函数的估算值是要足够精确的，而这需要红色的圈圈足够小（也就是学习率足够小）来保证。所以理论上每次更新参数都想要损失函数减小的话，即保证式1-2 成立的话，就需要学习率足够足够小才可以。</p>

<p>所以实际中，当更新参数的时候，如果学习率没有设好，是有可能式1-2是不成立的，所以导致做梯度下降的时候，损失函数没有越来越小。</p>

<blockquote>
  <p>式1-2只考虑了泰勒展开式的一次项，如果考虑到二次项（比如牛顿法），在实际中不是特别好，会涉及到二次微分等，多很多的运算，性价比不好。</p>
</blockquote>

<h1 id="梯度下降的限制">梯度下降的限制</h1>

<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0phhfnifj21200s0qet.jpg" alt="" /></p>

<ul>
  <li>容易陷入局部极值</li>
  <li>还有可能卡在不是极值，但微分值是0的地方</li>
  <li>还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</li>
</ul>

<blockquote>
  <p>新博客地址：<a href="http://yoferzhang.com/post/20170327ML04GradientDescent">http://yoferzhang.com/post/20170327ML04GradientDescent</a></p>
</blockquote>


        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2017/03/27/ML03BiasAndVariance/">机器学习入门系列03，Error的来源：偏差和方差（bias 和 variance）</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2017/03/27/Android_short_video_side_of_the_next_sowing_solution/">Android短视频边下边播详解</a></p>
        
    </div>
</div>


        <h2 id="comments">说一说</h2>
        

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript">
var uyan_config = {
     'url':'http://localhost:4000/2017/03/27/ML04GradientDescent/',
     'du':'http://localhost:4000', 
     'su':'http://localhost:4000/2017/03/27/ML04GradientDescent/' 
};
</script>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2136610"></script>
<!-- UY END -->






    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    目录
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">说一说</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
            
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/MelonTeam" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
            
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>


<script type="text/javascript" src="http://tajs.qq.com/stats?sId=62569168" charset="UTF-8"></script>


    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
