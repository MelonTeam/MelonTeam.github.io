<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>深度学习入门实战（二）</title>
    <meta name="description" content="  导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2017/03/28/Deep_learning_entry_2/">
    <link rel="alternate" type="application/rss+xml" title="MelonTeam" href="http://localhost:4000/feed.xml ">



</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">MelonTeam</a>
        <small>移动终端前沿技术的探索者</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/opensource/">
                        
                            <i class="fa fa-folder-open"></i>开源项目
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>归档
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>分类
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>标签
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/board/">
                        
                            <i class="fa fa-pencil"></i>留言板
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>深度学习入门实战（二）</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2017-03-28
            </div>
            
            <div class="label-card">
                <i class="fa fa-user"></i>chaodong
            </div>
            
            
            
            <div class="label-card">
            




<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#机器学习" title="Category: 机器学习" rel="category">机器学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>



            </div>
            
            
            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#tensorflow" title="Tag: tensorflow" rel="tag">tensorflow</a-->
        <a href="/tag/#tensorflow" title="Tag: tensorflow" rel="tag">tensorflow</a>&nbsp;
    
        <!--a href="/tag/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="Tag: 机器学习" rel="tag">机器学习</a-->
        <a href="/tag/#机器学习" title="Tag: 机器学习" rel="tag">机器学习</a>
    
  

</span>



            </div>
            

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        <ul id="markdown-toc">
  <li><a href="#0x00-概要" id="markdown-toc-0x00-概要">0x00 概要</a></li>
  <li><a href="#0x01-tensorflow安装" id="markdown-toc-0x01-tensorflow安装">0x01 TensorFlow安装</a></li>
  <li><a href="#0x02-tensorflow基本使用" id="markdown-toc-0x02-tensorflow基本使用">0x02 TensorFlow基本使用</a>    <ul>
      <li><a href="#1placehoder占位符" id="markdown-toc-1placehoder占位符">1.placehoder（占位符）</a></li>
      <li><a href="#2variable变量" id="markdown-toc-2variable变量">2.Variable（变量）</a></li>
      <li><a href="#3constant常量" id="markdown-toc-3constant常量">3.Constant（常量）</a></li>
      <li><a href="#4session会话" id="markdown-toc-4session会话">4.Session（会话）</a></li>
      <li><a href="#5简单使用" id="markdown-toc-5简单使用">5.简单使用</a></li>
    </ul>
  </li>
  <li><a href="#0x03-样例" id="markdown-toc-0x03-样例">0x03 样例</a></li>
</ul>
<blockquote>
  <p>导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～</p>
</blockquote>

<!--more-->

<h2 id="0x00-概要">0x00 概要</h2>

<p>TensorFlow是谷歌爸爸出的一个开源机器学习框架，目前已被广泛应用，谷歌爸爸出品即使性能不是最强的（其实性能也不错），但绝对是用起来最方便的，毕竟谷歌有Jeff Dean坐镇，这波稳。</p>

<h2 id="0x01-tensorflow安装">0x01 TensorFlow安装</h2>

<p>官方有一个Mac上TensorFlow的安装指南，点<a href="https://www.tensorflow.org/install/install_mac">这里</a>
我们现在就照着这个安装指南操作一把，官方推荐在virtualenv中安装TF，我们就在virtualenv安装吧，大家也可以直接安装。前几天TF发布1.0版了，我们就安装1.0版吧～</p>

<p>1.先安装下pip和six</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo easy_install --upgrade pip
<span class="gp">$ </span>sudo easy_install --upgrade six 
</code></pre>
</div>

<p>2.安装下virtualenv</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo pip install --upgrade virtualenv
</code></pre>
</div>

<p>3.接下来, 建立一个全新的 virtualenv 环境。这里将环境建在 ~/tensorflow目录下, 执行:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>virtualenv --system-site-packages ~/tensorflow
<span class="gp">$ </span><span class="nb">cd</span> ~/tensorflow
</code></pre>
</div>

<p>4.然后, 激活 virtualenv:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span><span class="nb">source </span>bin/activate  <span class="c"># 如果使用 bash</span>
<span class="gp">$ </span><span class="nb">source </span>bin/activate.csh  <span class="c"># 如果使用 csh</span>
</code></pre>
</div>

<p>(tensorflow)$ # 终端提示符应该发生变化
如果要退出虚拟环境可以执行</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">(tensorflow)$ </span>deactivate
</code></pre>
</div>

<p>也可以直接在shell里执行下面的代码激活</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">source</span> ~/tensorflow/bin/activate
</code></pre>
</div>

<p>5.在 virtualenv 内, 安装 TensorFlow:
因为我用的是Python 2.x所以执行</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>sudo pip install --upgrade  https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl
</code></pre>
</div>

<p>要是使用Python3可以执行</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code> <span class="nv">$ </span>pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl
</code></pre>
</div>

<p>当然也可以执行下面这个命令直接安装最新版</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>pip install --upgrade tensorflow
</code></pre>
</div>

<p>等命令执行完TF就安装好了</p>

<p>安装完成后可以在python中执行以下代码</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>import tensorflow as tf
hello <span class="o">=</span> tf.constant<span class="o">(</span><span class="s1">'Hello, TensorFlow!'</span><span class="o">)</span>
sess <span class="o">=</span> tf.Session<span class="o">()</span>
print<span class="o">(</span>sess.run<span class="o">(</span>hello<span class="o">))</span>
</code></pre>
</div>

<p>如果输出
Hello, TensorFlow!
就说明安装成功啦
PS:运行脚本的时候会提示不支持SSE xxx指令集的提示，这是因为我们是通过pip直接安装的编译好的版本导致的，如果想针对机器优化，可以直接从GitHub上的源代码编译安装。但这样会复杂些，而且我觉得其实提升不大，用CPU都很慢。。。不如直接上GPU性能提升快
PS2:如果想安装GPU版会复杂些，首先要有一块支持CUDA的N卡，再安装CUDA驱动啥的，各位看官可以谷歌一下查询相关资料。如果不想搜索，也可以看本系列后续文章，以后也会介绍如何在Mac下安装GPU版。</p>

<h2 id="0x02-tensorflow基本使用">0x02 TensorFlow基本使用</h2>

<p>在介绍样例之前，我们先介绍一下TensorFlow的一些基本概念</p>

<h3 id="1placehoder占位符">1.placehoder（占位符）</h3>

<blockquote>
  <p>tf.placeholder(dtype, shape=None, name=None)
  Args:
    dtype: The type of elements in the tensor to be fed.
    shape: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.
    name: A name for the operation (optional).</p>
</blockquote>

<p>dytpe:占位符的数据类型
shape:占位符的纬度，例如[2,2]代表2x2的二维矩阵，None可以代表任意维度，例如[None,2]则代表任意行数，2列的二维矩阵
name:占位符的名字</p>

<p>变量在定义时要初始化，但可能有些变量我们一开始定义的时候并不一定知道该变量的值，只有当真正开始运行程序的时候才由外部输入，比如我们需要训练的数据，所以就用占位符来占个位置，告诉TensorFlow，等到真正运行的时候再通过输入数据赋值。
例如</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</code></pre>
</div>

<p>就是生成了一个2x2的二维矩阵，矩阵中每个元素的类型都是tf.float32（也就是浮点型）
有时候定义需要训练的参数时候，会定义一个[input_size,output_size]大小的矩阵，其中input_size数输入数据的维度，output_size是输出数据的维度</p>

<h3 id="2variable变量">2.Variable（变量）</h3>

<p>官方说明 有些长，我就不引用啦，这里介绍一个简单的用法，有一点变量在声明的时候要有一个初始值</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span> <span class="c"># 声明一个2x2的矩阵，并将矩阵中的所有元素的值赋为0，默认每个元素都是tf.float32类型的数据</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c"># 声明一个tf.float32的变量，并将初始值设为1.0</span>
</code></pre>
</div>

<p>我们一般还需要运行下global_variables_initializer真正在TensorFlow的Session中初始化所有变量，后面的样例中也会有体现。</p>

<h3 id="3constant常量">3.Constant（常量）</h3>

<p><a href="https://www.tensorflow.org/api_docs/python/tf/Variable">官方说明</a> 同样不引用啦，这里介绍一个简单的用法</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c"># 定义一个值为3.0的浮点型常量</span>
</code></pre>
</div>

<h3 id="4session会话">4.Session（会话）</h3>

<p>TensorFlow所有的操作都必须在Session中运行，才能真正起作用，可以将Session当作TensorFlow运行的环境，Session运行完需要close～</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#用close()关闭</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">sess</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="c">#使用with..as..语句关闭</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
<span class="err">    </span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="5简单使用">5.简单使用</h3>

<p>我们介绍下3+5应该如何在TensorFlow中实现</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span> <span class="o">//</span> <span class="err">声明一个整型变量</span><span class="mi">3</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span> <span class="o">//</span> <span class="err">声明一个整型变量</span><span class="mi">5</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="o">//</span> <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span> <span class="o">//</span> <span class="err">初始化变量的操作</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>  <span class="o">//</span> <span class="err">在</span><span class="n">Session</span><span class="err">中初始化变量</span>
    <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> <span class="o">//</span> <span class="err">输出计算出的</span><span class="n">z</span><span class="err">值</span>
</code></pre>
</div>

<h2 id="0x03-样例">0x03 样例</h2>

<p>Github上有一个比较好的<a href="https://github.com/aymericdamien/TensorFlow-Examples">Demo合集</a>，有注释有源代码还蛮好的，但今天我们不讲上面的代码，我们讲如何用TF实现线性回归模型
所谓线性回归模型就是y = W * x + b的形式的表达式拟合的模型。
我们如果想通过深度学习拟合一条直线 y = 3 * x 应该怎么做呢？咱不讲虚的先展示下代码！然后我们在逐步分析。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#coding=utf-8</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">lost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_</span><span class="o">-</span><span class="n">y</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.0000001</span><span class="p">)</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">lost</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

<span class="n">steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span><span class="p">:</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">ys</span> <span class="p">}</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"After </span><span class="si">%</span><span class="s">d iteration:"</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"W: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"b: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"lost: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">lost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">))</span>
</code></pre>
</div>

<p>1.先导入需要使用的python库</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c">#coding=utf-8</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</code></pre>
</div>

<p>毕竟是基于TensorFlow的，那我们肯定要导入TensorFlow滴，导入之后取个别名tf，之后用起来方便些。</p>

<p>2.定义需要的变量，我们看看y = W * x + b中都有哪些变量</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</code></pre>
</div>

<p>x：我们训练时需要输入的真实数据x
W: 我们需要训练的W，这里我们定义了一个1维的变量（其实吧，就是一个普普通通的数，直接用tf.float32也行）并将其初值赋为0
b : 我们需要训练的b，定义一个1维变量，并将其初值赋为0
y_ ：我们训练时需要输入的x对应的y</p>

<p>3.定义线性模型</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
</code></pre>
</div>

<p>4.定义损失函数和优化方法</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">lost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_</span><span class="o">-</span><span class="n">y</span><span class="p">))</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.0000001</span><span class="p">)</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">lost</span><span class="p">)</span>
<span class="n">lost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_</span><span class="o">-</span> <span class="n">y</span><span class="p">))</span>
</code></pre>
</div>

<p>损失函数(Lost Function)是用来评估我们预测的值和真实的值之间的差距是多少，损失函数有很多种写法，我们这里使用（y预测-y真实)^2再取平均数来作为我们的损失函数（用这个函数是有原因的，因为我们用的是梯度下降法进行学习）损失函数的值越小越好，有些教程也叫Cost Function</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.0000001</span><span class="p">)</span>
</code></pre>
</div>

<p>优化函数代表我们要通过什么方式去优化我们需要学习的值，这个例子里指的是W和b，优化函数的种类有很多，大家到<a href="https://www.tensorflow.org/api_guides/python/train">官网</a>查阅，
平时我们用的比较多的是GradientDescentOptimizer和AdamOptimizer等，这里我们选用最常用也是最最基本的GradientDescentOptimizer（梯度下降），后面传入的值是学习效率。一般是一个小于1的数。越小收敛越慢，但并不是越大收敛越快哈，取值太大甚至可能不收敛了。。。
我们简单介绍下什么是梯度下降，梯度顾名思义就是函数某一点的导数，也就是该点的变化率。梯度下降则顾名思义就是沿梯度下降的方向求解极小值。
详细解释大家可以自行谷歌一下～当然可以可以看<a href="http://blog.csdn.net/yhao2014/article/details/51554910">这篇文章</a>，当然由于性能的原因梯度下降有很多种变种，例如随机梯度下降 (Stochastic Gradient Descent)，小批梯度下降 (Mini-Batch Gradient Descent)。本文样例采用的是SGD，每次只输入一个数据。</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">train_step</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">lost</span><span class="p">)</span>
</code></pre>
</div>

<p>这个代表我们每次训练迭代的目的，本例我们的目的就是尽量减小lost的值，也就是让损失函数的值尽量变小</p>

<p>5.变量初始化</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
</code></pre>
</div>

<p>这个之前有所介绍了，我们需要在Session中真正运行下global_variables_initializer才会真正初始化变量</p>

<p>6.开始训练</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span> <span class="n">x</span><span class="p">:</span> <span class="n">xs</span><span class="p">,</span> <span class="n">y_</span><span class="p">:</span> <span class="n">ys</span> <span class="p">}</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"After </span><span class="si">%</span><span class="s">d iteration:"</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"W: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"b: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"lost: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">lost</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed</span><span class="p">))</span>
</code></pre>
</div>

<p>我们定义一个训练迭代次数1000次
这里我们图方便，每次迭代都直接将i作为x，3*i作为y直接当成训练数据。
我们所有通过placeholder定义的值，在训练时我们都需要通过feed_dict来传入数据。
然后我们每隔100次迭代，输出一次训练结果，看看效果如何～</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">After</span> <span class="mi">0</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">0.000000</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.000000</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.000000</span>
<span class="n">After</span> <span class="mi">100</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">0.196407</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.002951</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">78599.671875</span>
<span class="n">After</span> <span class="mi">200</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">1.249361</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.009867</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">122582.625000</span>
<span class="n">After</span> <span class="mi">300</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.513344</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.015055</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">21310.636719</span>
<span class="n">After</span> <span class="mi">400</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.960238</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016392</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">252.449890</span>
<span class="n">After</span> <span class="mi">500</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.999347</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016484</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.096061</span>
<span class="n">After</span> <span class="mi">600</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.999971</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016485</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.000001</span>
<span class="n">After</span> <span class="mi">700</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.999975</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016485</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.000001</span>
<span class="n">After</span> <span class="mi">800</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.999978</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016485</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.000001</span>
<span class="n">After</span> <span class="mi">900</span> <span class="n">iteration</span><span class="p">:</span>
<span class="n">W</span><span class="p">:</span> <span class="mf">2.999981</span>
<span class="n">b</span><span class="p">:</span> <span class="mf">0.016485</span>
<span class="n">lost</span><span class="p">:</span> <span class="mf">0.000000</span>
</code></pre>
</div>

<p>可以看到在迭代了500次之后效果就很好了，w已经达到2.999347很接近3了，b也达到了0.016484也比较接近0了，因为这里学习率选择的比较小，所以收敛的比较慢，各位也可以尝试调大学习率，看看收敛的速度有何变化。</p>

        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2017/03/28/Deep_learning_entry_1/">深度学习入门实战（一）</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2017/04/05/ji_qi_xue_xi_ru_men_xi_lie_05_classification__probabilistic_generative_model__fen_lei___gai_lv_sheng_cheng_mo_xing__/">机器学习入门系列05，classification: probabilistic generative model（分类：概率生成模型）</a></p>
        
    </div>
</div>


        <h2 id="comments">说一说</h2>
        

<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript">
var uyan_config = {
     'url':'http://localhost:4000/2017/03/28/Deep_learning_entry_2/',
     'du':'http://localhost:4000', 
     'su':'http://localhost:4000/2017/03/28/Deep_learning_entry_2/' 
};
</script>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2136610"></script>
<!-- UY END -->






    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    目录
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#comments">说一说</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
            
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/MelonTeam" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>         
            
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>


<script type="text/javascript" src="http://tajs.qq.com/stats?sId=62569168" charset="UTF-8"></script>


    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
