<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>深度学习入门实战（二） | Melon Team</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门实战（二）">
<meta property="og:url" content="http://melonteam.com/2017/03/28/Deep_learning_entry_2/index.html">
<meta property="og:site_name" content="Melon Team">
<meta property="og:description" content="导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～">
<meta property="og:updated_time" content="2017-03-28T02:28:19.370Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习入门实战（二）">
<meta name="twitter:description" content="导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～">
  
    <link rel="alternate" href="/atom.xml" title="Melon Team" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Melon Team</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://melonteam.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Deep_learning_entry_2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/28/Deep_learning_entry_2/" class="article-date">
  <time datetime="2017-03-28T03:04:58.000Z" itemprop="datePublished">2017-03-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习入门实战（二）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>导语：上一篇文章我们介绍了MxNet的安装，但MxNet有个缺点，那就是文档不太全，用起来可能是要看源代码才能理解某个方法的含义，所以今天我们就介绍一下TensorFlow，这个由谷歌爸爸出品的深度学习框架，文档比较全～以后的我们也都使用这个框架～</p>
</blockquote>
<a id="more"></a>
<h2 id="0x00-概要"><a href="#0x00-概要" class="headerlink" title="0x00 概要"></a>0x00 概要</h2><p>TensorFlow是谷歌爸爸出的一个开源机器学习框架，目前已被广泛应用，谷歌爸爸出品即使性能不是最强的（其实性能也不错），但绝对是用起来最方便的，毕竟谷歌有Jeff Dean坐镇，这波稳。</p>
<h2 id="0x01-TensorFlow安装"><a href="#0x01-TensorFlow安装" class="headerlink" title="0x01 TensorFlow安装"></a>0x01 TensorFlow安装</h2><p>官方有一个Mac上TensorFlow的安装指南，点<a href="https://www.tensorflow.org/install/install_mac" target="_blank" rel="external">这里</a><br>我们现在就照着这个安装指南操作一把，官方推荐在virtualenv中安装TF，我们就在virtualenv安装吧，大家也可以直接安装。前几天TF发布1.0版了，我们就安装1.0版吧～</p>
<p>1.先安装下pip和six</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo easy_install --upgrade pip</div><div class="line">$ sudo easy_install --upgrade six</div></pre></td></tr></table></figure>
<p>2.安装下virtualenv</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo pip install --upgrade virtualenv</div></pre></td></tr></table></figure>
<p>3.接下来, 建立一个全新的 virtualenv 环境。这里将环境建在 ~/tensorflow目录下, 执行:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ virtualenv --system-site-packages ~/tensorflow</div><div class="line">$ <span class="built_in">cd</span> ~/tensorflow</div></pre></td></tr></table></figure>
<p>4.然后, 激活 virtualenv:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">source</span> bin/activate  <span class="comment"># 如果使用 bash</span></div><div class="line">$ <span class="built_in">source</span> bin/activate.csh  <span class="comment"># 如果使用 csh</span></div></pre></td></tr></table></figure>
<p>(tensorflow)$ # 终端提示符应该发生变化<br>如果要退出虚拟环境可以执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ deactivate</div></pre></td></tr></table></figure>
<p>也可以直接在shell里执行下面的代码激活</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> ~/tensorflow/bin/activate</div></pre></td></tr></table></figure>
<p>5.在 virtualenv 内, 安装 TensorFlow:<br>因为我用的是Python 2.x所以执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo pip install --upgrade  https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py2-none-any.whl</div></pre></td></tr></table></figure>
<p>要是使用Python3可以执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0-py3-none-any.whl</div></pre></td></tr></table></figure>
<p>当然也可以执行下面这个命令直接安装最新版</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install --upgrade tensorflow</div></pre></td></tr></table></figure>
<p>等命令执行完TF就安装好了</p>
<p>安装完成后可以在python中执行以下代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</div><div class="line">sess = tf.Session()</div><div class="line"><span class="built_in">print</span>(sess.run(hello))</div></pre></td></tr></table></figure>
<p>如果输出<br>Hello, TensorFlow!<br>就说明安装成功啦<br>PS:运行脚本的时候会提示不支持SSE xxx指令集的提示，这是因为我们是通过pip直接安装的编译好的版本导致的，如果想针对机器优化，可以直接从GitHub上的源代码编译安装。但这样会复杂些，而且我觉得其实提升不大，用CPU都很慢。。。不如直接上GPU性能提升快<br>PS2:如果想安装GPU版会复杂些，首先要有一块支持CUDA的N卡，再安装CUDA驱动啥的，各位看官可以谷歌一下查询相关资料。如果不想搜索，也可以看本系列后续文章，以后也会介绍如何在Mac下安装GPU版。</p>
<h2 id="0x02-TensorFlow基本使用"><a href="#0x02-TensorFlow基本使用" class="headerlink" title="0x02 TensorFlow基本使用"></a>0x02 TensorFlow基本使用</h2><p>在介绍样例之前，我们先介绍一下TensorFlow的一些基本概念</p>
<h3 id="1-placehoder（占位符）"><a href="#1-placehoder（占位符）" class="headerlink" title="1.placehoder（占位符）"></a>1.placehoder（占位符）</h3><blockquote>
<p>tf.placeholder(dtype, shape=None, name=None)<br>  Args:<br>    dtype: The type of elements in the tensor to be fed.<br>    shape: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.<br>    name: A name for the operation (optional).</p>
</blockquote>
<p>dytpe:占位符的数据类型<br>shape:占位符的纬度，例如[2,2]代表2x2的二维矩阵，None可以代表任意维度，例如[None,2]则代表任意行数，2列的二维矩阵<br>name:占位符的名字</p>
<p>变量在定义时要初始化，但可能有些变量我们一开始定义的时候并不一定知道该变量的值，只有当真正开始运行程序的时候才由外部输入，比如我们需要训练的数据，所以就用占位符来占个位置，告诉TensorFlow，等到真正运行的时候再通过输入数据赋值。<br>例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, [<span class="number">2</span>, <span class="number">2</span>])</div></pre></td></tr></table></figure>
<p>就是生成了一个2x2的二维矩阵，矩阵中每个元素的类型都是tf.float32（也就是浮点型）<br>有时候定义需要训练的参数时候，会定义一个[input_size,output_size]大小的矩阵，其中input_size数输入数据的维度，output_size是输出数据的维度</p>
<h3 id="2-Variable（变量）"><a href="#2-Variable（变量）" class="headerlink" title="2.Variable（变量）"></a>2.Variable（变量）</h3><p>官方说明 有些长，我就不引用啦，这里介绍一个简单的用法，有一点变量在声明的时候要有一个初始值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.Variable(tf.zeros([<span class="number">2</span>,<span class="number">2</span>])) <span class="comment"># 声明一个2x2的矩阵，并将矩阵中的所有元素的值赋为0，默认每个元素都是tf.float32类型的数据</span></div><div class="line">y = tf.Variable(<span class="number">1.0</span>, tf.float32) <span class="comment"># 声明一个tf.float32的变量，并将初始值设为1.0</span></div></pre></td></tr></table></figure>
<p>我们一般还需要运行下global_variables_initializer真正在TensorFlow的Session中初始化所有变量，后面的样例中也会有体现。</p>
<h3 id="3-Constant（常量）"><a href="#3-Constant（常量）" class="headerlink" title="3.Constant（常量）"></a>3.Constant（常量）</h3><p><a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external">官方说明</a> 同样不引用啦，这里介绍一个简单的用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x = tf.constant(<span class="number">3.0</span>, tf.float32) <span class="comment"># 定义一个值为3.0的浮点型常量</span></div></pre></td></tr></table></figure>
<h3 id="4-Session（会话）"><a href="#4-Session（会话）" class="headerlink" title="4.Session（会话）"></a>4.Session（会话）</h3><p>TensorFlow所有的操作都必须在Session中运行，才能真正起作用，可以将Session当作TensorFlow运行的环境，Session运行完需要close～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#用close()关闭</span></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(...)</div><div class="line">sess.close()</div><div class="line"></div><div class="line"><span class="comment">#使用with..as..语句关闭</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(...)</div></pre></td></tr></table></figure>
<h3 id="5-简单使用"><a href="#5-简单使用" class="headerlink" title="5.简单使用"></a>5.简单使用</h3><p>我们介绍下3+5应该如何在TensorFlow中实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">x = tf.Variable(<span class="number">3</span>, tf.int16) // 声明一个整型变量<span class="number">3</span></div><div class="line">y = tf.Variable(<span class="number">5</span>, tf.int16) // 声明一个整型变量<span class="number">5</span></div><div class="line">z = tf.add(x,y) // z = x + y</div><div class="line">init = tf.global_variables_initializer() // 初始化变量的操作</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)  // 在Session中初始化变量</div><div class="line">    print(sess.run(z)) // 输出计算出的z值</div></pre></td></tr></table></figure>
<h2 id="0x03-样例"><a href="#0x03-样例" class="headerlink" title="0x03 样例"></a>0x03 样例</h2><p>Github上有一个比较好的<a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="external">Demo合集</a>，有注释有源代码还蛮好的，但今天我们不讲上面的代码，我们讲如何用TF实现线性回归模型<br>所谓线性回归模型就是y = W <em> x + b的形式的表达式拟合的模型。<br>我们如果想通过深度学习拟合一条直线 y = 3 </em> x 应该怎么做呢？咱不讲虚的先展示下代码！然后我们在逐步分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">x = tf.placeholder(tf.float32)</div><div class="line">W = tf.Variable(tf.zeros([<span class="number">1</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</div><div class="line">y_ = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">y = W * x + b</div><div class="line"></div><div class="line">lost = tf.reduce_mean(tf.square(y_-y))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.0000001</span>)</div><div class="line">train_step = optimizer.minimize(lost)</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">steps = <span class="number">1000</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</div><div class="line">    xs = [i]</div><div class="line">    ys = [<span class="number">3</span> * i]</div><div class="line">    feed = &#123; x: xs, y_: ys &#125;</div><div class="line">    sess.run(train_step, feed_dict=feed)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> :</div><div class="line">        print(<span class="string">"After %d iteration:"</span> % i)</div><div class="line">        print(<span class="string">"W: %f"</span> % sess.run(W))</div><div class="line">        print(<span class="string">"b: %f"</span> % sess.run(b))</div><div class="line">        print(<span class="string">"lost: %f"</span> % sess.run(lost, feed_dict=feed))</div></pre></td></tr></table></figure>
<p>1.先导入需要使用的python库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>毕竟是基于TensorFlow的，那我们肯定要导入TensorFlow滴，导入之后取个别名tf，之后用起来方便些。</p>
<p>2.定义需要的变量，我们看看y = W * x + b中都有哪些变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32)</div><div class="line">W = tf.Variable(tf.zeros([<span class="number">1</span>]))</div><div class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</div><div class="line">y_ = tf.placeholder(tf.float32)</div></pre></td></tr></table></figure>
<p>x：我们训练时需要输入的真实数据x<br>W: 我们需要训练的W，这里我们定义了一个1维的变量（其实吧，就是一个普普通通的数，直接用tf.float32也行）并将其初值赋为0<br>b : 我们需要训练的b，定义一个1维变量，并将其初值赋为0<br>y_ ：我们训练时需要输入的x对应的y</p>
<p>3.定义线性模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y = W * x + b</div></pre></td></tr></table></figure>
<p>4.定义损失函数和优化方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">lost = tf.reduce_mean(tf.square(y_-y))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.0000001</span>)</div><div class="line">train_step = optimizer.minimize(lost)</div><div class="line">lost = tf.reduce_mean(tf.square(y_- y))</div></pre></td></tr></table></figure>
<p>损失函数(Lost Function)是用来评估我们预测的值和真实的值之间的差距是多少，损失函数有很多种写法，我们这里使用（y预测-y真实)^2再取平均数来作为我们的损失函数（用这个函数是有原因的，因为我们用的是梯度下降法进行学习）损失函数的值越小越好，有些教程也叫Cost Function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.0000001</span>)</div></pre></td></tr></table></figure>
<p>优化函数代表我们要通过什么方式去优化我们需要学习的值，这个例子里指的是W和b，优化函数的种类有很多，大家到<a href="https://www.tensorflow.org/api_guides/python/train" target="_blank" rel="external">官网</a>查阅，<br>平时我们用的比较多的是GradientDescentOptimizer和AdamOptimizer等，这里我们选用最常用也是最最基本的GradientDescentOptimizer（梯度下降），后面传入的值是学习效率。一般是一个小于1的数。越小收敛越慢，但并不是越大收敛越快哈，取值太大甚至可能不收敛了。。。<br>我们简单介绍下什么是梯度下降，梯度顾名思义就是函数某一点的导数，也就是该点的变化率。梯度下降则顾名思义就是沿梯度下降的方向求解极小值。<br>详细解释大家可以自行谷歌一下～当然可以可以看<a href="http://blog.csdn.net/yhao2014/article/details/51554910" target="_blank" rel="external">这篇文章</a>，当然由于性能的原因梯度下降有很多种变种，例如随机梯度下降 (Stochastic Gradient Descent)，小批梯度下降 (Mini-Batch Gradient Descent)。本文样例采用的是SGD，每次只输入一个数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = optimizer.minimize(lost)</div></pre></td></tr></table></figure>
<p>这个代表我们每次训练迭代的目的，本例我们的目的就是尽量减小lost的值，也就是让损失函数的值尽量变小</p>
<p>5.变量初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">init = tf.global_variables_initializer()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure>
<p>这个之前有所介绍了，我们需要在Session中真正运行下global_variables_initializer才会真正初始化变量</p>
<p>6.开始训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">steps = <span class="number">1000</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</div><div class="line">    xs = [i]</div><div class="line">    ys = [<span class="number">3</span> * i]</div><div class="line">    feed = &#123; x: xs, y_: ys &#125;</div><div class="line">    sess.run(train_step, feed_dict=feed)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span> :</div><div class="line">        print(<span class="string">"After %d iteration:"</span> % i)</div><div class="line">        print(<span class="string">"W: %f"</span> % sess.run(W))</div><div class="line">        print(<span class="string">"b: %f"</span> % sess.run(b))</div><div class="line">        print(<span class="string">"lost: %f"</span> % sess.run(lost, feed_dict=feed))</div></pre></td></tr></table></figure>
<p>我们定义一个训练迭代次数1000次<br>这里我们图方便，每次迭代都直接将i作为x，3*i作为y直接当成训练数据。<br>我们所有通过placeholder定义的值，在训练时我们都需要通过feed_dict来传入数据。<br>然后我们每隔100次迭代，输出一次训练结果，看看效果如何～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">After <span class="number">0</span> iteration:</div><div class="line">W: <span class="number">0.000000</span></div><div class="line">b: <span class="number">0.000000</span></div><div class="line">lost: <span class="number">0.000000</span></div><div class="line">After <span class="number">100</span> iteration:</div><div class="line">W: <span class="number">0.196407</span></div><div class="line">b: <span class="number">0.002951</span></div><div class="line">lost: <span class="number">78599.671875</span></div><div class="line">After <span class="number">200</span> iteration:</div><div class="line">W: <span class="number">1.249361</span></div><div class="line">b: <span class="number">0.009867</span></div><div class="line">lost: <span class="number">122582.625000</span></div><div class="line">After <span class="number">300</span> iteration:</div><div class="line">W: <span class="number">2.513344</span></div><div class="line">b: <span class="number">0.015055</span></div><div class="line">lost: <span class="number">21310.636719</span></div><div class="line">After <span class="number">400</span> iteration:</div><div class="line">W: <span class="number">2.960238</span></div><div class="line">b: <span class="number">0.016392</span></div><div class="line">lost: <span class="number">252.449890</span></div><div class="line">After <span class="number">500</span> iteration:</div><div class="line">W: <span class="number">2.999347</span></div><div class="line">b: <span class="number">0.016484</span></div><div class="line">lost: <span class="number">0.096061</span></div><div class="line">After <span class="number">600</span> iteration:</div><div class="line">W: <span class="number">2.999971</span></div><div class="line">b: <span class="number">0.016485</span></div><div class="line">lost: <span class="number">0.000001</span></div><div class="line">After <span class="number">700</span> iteration:</div><div class="line">W: <span class="number">2.999975</span></div><div class="line">b: <span class="number">0.016485</span></div><div class="line">lost: <span class="number">0.000001</span></div><div class="line">After <span class="number">800</span> iteration:</div><div class="line">W: <span class="number">2.999978</span></div><div class="line">b: <span class="number">0.016485</span></div><div class="line">lost: <span class="number">0.000001</span></div><div class="line">After <span class="number">900</span> iteration:</div><div class="line">W: <span class="number">2.999981</span></div><div class="line">b: <span class="number">0.016485</span></div><div class="line">lost: <span class="number">0.000000</span></div></pre></td></tr></table></figure>
<p>可以看到在迭代了500次之后效果就很好了，w已经达到2.999347很接近3了，b也达到了0.016484也比较接近0了，因为这里学习率选择的比较小，所以收敛的比较慢，各位也可以尝试调大学习率，看看收敛的速度有何变化。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://melonteam.com/2017/03/28/Deep_learning_entry_2/" data-id="cj0ur7ar90001sc1u8y5c3rr3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/03/28/Deep_learning_entry_1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">深度学习入门实战（一）</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IM/">IM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IOS/">IOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/我说/">我说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/视频/">视频</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_2/">深度学习入门实战（二）</a>
          </li>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_1/">深度学习入门实战（一）</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Video_codec_learning_sharing/">视频编解码学习分享</a>
          </li>
        
          <li>
            <a href="/2017/03/27/IOS_touch_event_distribution_mechanism/">IOS触摸事件分发机制详解</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Android_short_video_side_of_the_next_sowing_solution/">Android短视频边下边播详解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Melon Team<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>