<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习入门系列02，Regression 回归：案例研究 | Melon Team</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66577532

为什么要先进行案例研究">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习入门系列02，Regression 回归：案例研究">
<meta property="og:url" content="http://melonteam.com/2017/03/27/ML02Regression/index.html">
<meta property="og:site_name" content="Melon Team">
<meta property="og:description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66577532

为什么要先进行案例研究">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0me8ssa8j20zc0kcn5j.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mf7rxiij20xo0hgn53.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mfqcewwj211i0jiwhn.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mg9bqyyj210o0o678o.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mgsxi8wj212i0ocjwj.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mh9cr4mj21220najwa.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mhq93m2j210c0lg0w5.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mi9ztuvj20y80maqnz.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mir5gsdj212c0lo7qj.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mmwc0tgj212m0l4tck.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnf5r0uj212o0lmn22.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnwbr3gj21360tgn32.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0moh17mgj213g0tsgs2.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0moxjsx4j21380to44a.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpebvrmj213c0twjx7.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpwdlv6j212y0soafr.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqdlyzij212o0m6adw.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqsw1iuj20y40ms7ff.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mr9qf2mj20yo0joq51.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mrpzq6hj21bm0u0qof.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0ms6u00hj21020t6gs0.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mspg7tjj213k0sgdkf.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mt6esluj213c0m6788.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mtp38ryj212g0smte8.jpg">
<meta property="og:updated_time" content="2017-03-28T07:44:34.170Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习入门系列02，Regression 回归：案例研究">
<meta name="twitter:description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66577532

为什么要先进行案例研究">
<meta name="twitter:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0me8ssa8j20zc0kcn5j.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Melon Team" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Melon Team</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://melonteam.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-ML02Regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/27/ML02Regression/" class="article-date">
  <time datetime="2017-03-27T02:02:44.000Z" itemprop="datePublished">2017-03-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习入门系列02，Regression 回归：案例研究
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>引用课程：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="external">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></p>
</blockquote>
<p>先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）</p>
<blockquote>
<p>CSDN博客文章地址：<a href="http://blog.csdn.net/zyq522376829/article/details/66577532" target="_blank" rel="external">http://blog.csdn.net/zyq522376829/article/details/66577532</a></p>
</blockquote>
<h1 id="为什么要先进行案例研究？"><a href="#为什么要先进行案例研究？" class="headerlink" title="为什么要先进行案例研究？"></a>为什么要先进行案例研究？</h1><p>没有比较好的数学基础，直接接触深度学习会非常抽象，所以这里我们先通过一个预测 Pokemon Go 的 Combat Power (CP) 值的案例，打开深度学习的大门。</p>
<h1 id="Regression-（回归）"><a href="#Regression-（回归）" class="headerlink" title="Regression （回归）"></a>Regression （回归）</h1><a id="more"></a>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0me8ssa8j20zc0kcn5j.jpg" alt=""></p>
<h2 id="应用举例（预测Pokemon-Go-进化后的战斗力）"><a href="#应用举例（预测Pokemon-Go-进化后的战斗力）" class="headerlink" title="应用举例（预测Pokemon Go 进化后的战斗力）"></a>应用举例（预测Pokemon Go 进化后的战斗力）</h2><p>比如估计一只神奇宝贝进化后的 CP 值（战斗力）。<br>下面是一只妙蛙种子，可以进化为妙蛙草，现在的CP值是14，我们想估计进化后的CP值是多少；进化需要糖果，好处就是如果它进化后CP值不满意，那就不用浪费糖果来进化它了，可以选择性价比高的神奇宝贝。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mf7rxiij20xo0hgn53.jpg" alt=""></p>
<p>输入用了一些不同的 $x$ 来代表不同的属性，比如战斗力用 $x<em>{cp}$ 来表示，物种 $x</em>{s}$ 来表示…<br>输出就是进化后的CP值</p>
<h3 id="三个步骤"><a href="#三个步骤" class="headerlink" title="三个步骤"></a>三个步骤</h3><p>上一篇提到了机器学习的三个步骤：<br>Step1.确定一组函数（Model）。<br>Step2.将训练集对函数集进行训练。<br>Step3.挑选出“最好”的函数 $f^{<em>}$<br>然后就可以使用 $f^{</em>}$ 来对新的测试集进行检测。</p>
<h3 id="Step1-Model"><a href="#Step1-Model" class="headerlink" title="Step1: Model"></a>Step1: Model</h3><p>这个model 应该长什么样子呢，先写一个简单的：我们可以认为进化后的CP值 $y$ 等于进化前的CP值 $x_{cp}$ 乘以一个参数 $w$ 再加上一个参数 $b$ 。</p>
<p>$$<br>y = b + w \cdot x_{cp}   \qquad (1-1)<br>$$</p>
<p>$w$ 和 $b$ 是参数，可以是任何数值。</p>
<p>可以有<br>$$<br>f<em>{1}: y = 10.0 + 9.0 \cdot x</em>{cp}<br>$$</p>
<p>$$<br>f<em>{2}: y = 9.8 + 9.2 \cdot x</em>{cp}<br>$$</p>
<p>$$<br>f<em>{3}: y = -0.8 -1.2 \cdot x</em>{cp}<br>$$</p>
<p>这个函数集中可以有无限多的 function。所以我们用 $y = b + w \cdot x<em>{cp} $ 代表这些 function 所成的集合。还有比如上面的 $f</em>{3}$ ，明显是不正确的，因为CP值有个条件都是正的，那乘以 $-1.2$ 就变成负的了，所以我们接着就要根据训练集来找到，这个 function set 里面，哪个是合理的 function。</p>
<p>我们将式1-1 称作 Linear model， Linear model 形式为：</p>
<p>$$<br>y = b + \sum w<em>{i}x</em>{i}<br>$$</p>
<p>$x<em>{i}$ 就是神奇宝贝的各种不同的属性，身高、体重等等，我们将这些称之为 “feature（特征）”；$w</em>{i}$ 称为 weight（权重），b 称为 bias（偏差）。</p>
<h3 id="Step2-方程的好坏"><a href="#Step2-方程的好坏" class="headerlink" title="Step2: 方程的好坏"></a>Step2: 方程的好坏</h3><p>现在就需要搜集训练集，这里的数据集是 Supervised 的，所以需要 function 的输入和输出（数值），举例抓了一只杰尼龟，进化前的CP值为612，用 $x^{1}$ 代表这只杰尼龟进化前的CP值，即用上标标示一个完整对象的编号；进化后的CP值为 979，用 $\hat{y}^{1}$ 表示进化后的CP值，用 hat（字母头顶的上尖符号）来表示这是一个正确的值，是实际观察到function该有的输出。</p>
<p>下面我们来看真正的数据集（来源 Source: <a href="https://www.openintro.org/stat/data/?data=pokemon" target="_blank" rel="external">https://www.openintro.org/stat/data/?data=pokemon</a>）</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mfqcewwj211i0jiwhn.jpg" alt=""></p>
<p>来看10只神奇宝贝的真实数据，$x$ 轴代表进化前的CP值，$y$ 轴代表进化后的CP值。</p>
<p>有了训练集，为了评价 function 的好坏，我们需要定义一个新的函数，称为 <strong>Loss function (损失函数)</strong>，定义如下：</p>
<p>Loss function $L$ :</p>
<p>input: a function, output: how bad it is</p>
<p>Loss function是比较特别的函数，是函数的函数，因为它的输入是一个函数，而输出是表示输入的函数有多不好。 可以写成下面这种形式：</p>
<p>$$<br>L(f) = L(w, b)<br>$$</p>
<p>损失函数是由一组参数 w和b决定的，所以可以说损失函数是在衡量一组参数的好坏。</p>
<p>这里用比较常见的定义形式：</p>
<p>$$<br>L(f) = L(w, b) =\sum<em>{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x</em>{cp}^{n})\right)^{2} \qquad   (1-2)<br>$$</p>
<p>将实际的数值 $\hat{y}^{n}$ 减去 估测的数值 $b + w\cdot x_{cp}^{n}$，然后再给平方，就是 Estimation error（估测误差，总偏差）；最后将估测误差加起来就是我们定义的损失函数。</p>
<p>这里不取各个偏差的代数和$\sum<em>{n=1}^{10}\hat{y}^{n} -(b + w\cdot x</em>{cp}^{n})$ 作为总偏差，这是因为这些偏差（$\hat{y}^{i} -(b + w\cdot x_{cp}^{i})$）本身有正有负，如果简单地取它们的代数和，就可能互相抵消，这是虽然偏差的代数和很小，却不能保证各个偏差都很小。所以按照式1-2，是这些偏差的平方和最小，就可以保证每一个偏差都很小。</p>
<p>为了更加直观，来对损失函数进行作图：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mg9bqyyj210o0o678o.jpg" alt=""></p>
<p>图上每个点都代表一个方程，比如红色的那个点代表 $y=-180-2\cdot x_{cp}$ 。颜色代表用这个点的方程得到的损失函数有多不好，颜色越偏红色，代表数值越大，越偏蓝色蓝色，代表方程越好。最好的方程就是图中叉叉标记的点。</p>
<h3 id="Step3：最好的方程"><a href="#Step3：最好的方程" class="headerlink" title="Step3：最好的方程"></a>Step3：最好的方程</h3><p>定好了损失函数，可以衡量每一个方程的好坏，接下来需要从函数集中挑选一个最好的方程。将这个过程数学化：</p>
<p>$$<br>f^{*}=\arg \min_{f} L(f)<br>$$</p>
<p>$$<br>w^{<em>},b^{</em>} = \arg \min<em>{w,b} L(w,b)=\arg \min</em>{w,b} \sum<em>{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x</em>{cp}^{n})\right)^{2} \qquad  (1-3)<br>$$</p>
<p>由于这里举例的特殊性，对于式1-3，直接使用最小二乘法即可解出最优的 w 和 b，使得总偏差最小。</p>
<blockquote>
<p>简单说一下<strong>最小二乘法</strong>，对于二元函数 $f(x,y)$，函数的极值点必为 $\frac{\partial f}{\partial x}$ 及$\frac{\partial f}{\partial y}$ 同时为零或至少有一个偏导数不存在的点；这是极值的必要条件。用这个极值条件可以解出w 和 b。（详情请参阅《数学分析，第三版下册，欧阳光中 等编》第十五章，第一节）</p>
</blockquote>
<p>但这里会使用另外一种做法，<strong>Gradient Descent（最速下降法）</strong>，最速下降法不光能解决式1-3 这一种问题；实际上只要 $L$ 是可微分的，都可以用最速下降法来处理。</p>
<h3 id="Gradient-Descent（梯度下降法）"><a href="#Gradient-Descent（梯度下降法）" class="headerlink" title="Gradient Descent（梯度下降法）"></a>Gradient Descent（梯度下降法）</h3><p>简单来看一下梯度下降法的做法。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mgsxi8wj212i0ocjwj.jpg" alt=""></p>
<p>考虑只有一个参数 $w$ 的损失函数，随机的选取一个初始点，计算 $w = w^{0}$ 时 $L$ 对 $w$ 的微分，然后顺着切线下降的方向更改 $w$ 的值（因为这里是求极小值），即斜率为负，增加$w$ ；斜率为正，减小$w$ .</p>
<p>那么每次更改 $w$ ，更改多大，用 $\eta \frac{\mathrm{d}L}{\mathrm{d}w} |_{w=w^{0}}$ 表示，$\eta$ 被称为“learning rate”学习速率。</p>
<p>由于这里斜率是负的，所以是 $w^{0} - \eta \frac{\mathrm{d}L}{\mathrm{d}w} |_{w=w^{0}}$ ，得到 $w^{1}$；接着就是重复上述步骤。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mh9cr4mj21220najwa.jpg" alt=""></p>
<p>直到找到一个点，这个点的斜率为0。但是例子中的情况会比较疑惑，这样的方法很可能找到的只是局部极值，并不是全局极值，但这是由于我们例子的原因，针对回归问题来说，是不存在局部极值的，只有全局极值。所以这个方法还是可以使用。</p>
<p>下面来看看两个参数的问题。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mhq93m2j210c0lg0w5.jpg" alt=""></p>
<p>两个参数的区别就是每次需要对两个参数求偏微分，然后同理更新参数的值。</p>
<blockquote>
<p>关于梯度可以参阅《数学分析，第三版下册，欧阳光中 等编》，第十四章第六节。也可以大概看看<a href="http://baike.baidu.com/link?url=zcPz3cjP_TPVZOHaLK49CD8CBSRJgD2ZyJ8fq9FOlqB6zX1NHw5p5ilA-CDTc87ujD3waWQygEnhPL7ERofrbI7AhRWLMDKTCtLXjbhFvve" target="_blank" rel="external">百度百科</a>又或者<a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="external">wikipedia</a></p>
</blockquote>
<p>将上述做法可视化：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mi9ztuvj20y80maqnz.jpg" alt=""></p>
<p>同理梯度下降的缺陷如下图：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mir5gsdj212c0lo7qj.jpg" alt=""></p>
<p>可能只是找到了局部极值，但是对于线性回归，可以保证所选取的损失函数式1-2是 convex（凸的，即只存在唯一极值）。上图右边就是损失函数的等高线图，可以看出是一圈一圈向内减小的。</p>
<h2 id="结果怎么样呢？"><a href="#结果怎么样呢？" class="headerlink" title="结果怎么样呢？"></a>结果怎么样呢？</h2><p>将求出的结果绘图如下</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0mmwc0tgj212m0l4tck.jpg" alt=""></p>
<p>可以计算出训练集上的偏差绝对值之和为 31.9</p>
<p>但真正关心的并不是在训练集上的偏差，而是Generalization的情况，就是需要在新的数据集（测试集）上来计算偏差。如下图：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnf5r0uj212o0lmn22.jpg" alt=""></p>
<p>使用十个新的神奇宝贝的数据作为测试集计算出偏差绝对值之和为35.</p>
<p>接下来考虑是否能够做的更好，可能并不只是简单的直线，考虑其他model的情况：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mnwbr3gj21360tgn32.jpg" alt=""></p>
<p>比如重新设计一个model，多一个二次项，来求出参数，得到Average Error为15.4，在训练集上看起来更好了。在测试集上得出的Average Error是18.4，确实是更好的Model。</p>
<p>再考虑三次项：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0moh17mgj213g0tsgs2.jpg" alt=""></p>
<p>得到的结果看起来和二次项时候的结果差别不大，稍微好一点点。也可以看到$w_{3}$已经非常小了，说明三次项影响已经不大了。</p>
<p>再考虑四次项：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0moxjsx4j21380to44a.jpg" alt=""></p>
<p>此时在训练集上可以做的更好，但是测试集的结果变差了。</p>
<p>再考虑五次项：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpebvrmj213c0twjx7.jpg" alt=""></p>
<p>可以看到测试集的结果非常差。</p>
<h2 id="Overfitting（过拟合，过度学习）"><a href="#Overfitting（过拟合，过度学习）" class="headerlink" title="Overfitting（过拟合，过度学习）"></a>Overfitting（过拟合，过度学习）</h2><p>将训练集上的Average Error变化进行作图：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mpwdlv6j212y0soafr.jpg" alt=""></p>
<p>可以看到训练集上的 Average Error 逐渐变小。</p>
<p>上面的那些model，高次项是包含低次项的function。理论上确实次幂越高越复杂的方程，可以让训练集的结果越低。但加上测试集的结果：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqdlyzij212o0m6adw.jpg" alt=""></p>
<p>观察得出结果：虽然越复杂的model可以在训练集上得到更好的结果，但越复杂的model并不一定在测试集上有好的结果。这个结论叫做“<strong>Overfitting（过拟合）</strong>”。</p>
<p>如果此时要选model的话，最好的选择就是三次项式子的model。</p>
<blockquote>
<p>实际生活中典型的学驾照，学驾照的时候在驾校的训练集上人们可以做的很好，但上路之后真正的测试集就完全无法驾驭。这里只是举个训练集很好，而测试集结果很差的例子^_^</p>
</blockquote>
<h2 id="如果数据更多会怎样？"><a href="#如果数据更多会怎样？" class="headerlink" title="如果数据更多会怎样？"></a>如果数据更多会怎样？</h2><p>考虑60只神奇宝贝的数据</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mqsw1iuj20y40ms7ff.jpg" alt=""></p>
<p>可以看出物种也是一个关键性的因素，只考虑进化前的CP值是太局限的，刚才的model就设计的不太好。</p>
<p>新的model如下</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0mr9qf2mj20yo0joq51.jpg" alt=""></p>
<p>将这个model写成linear model的形式：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mrpzq6hj21bm0u0qof.jpg" alt=""></p>
<p>来看做出来的结果：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0ms6u00hj21020t6gs0.jpg" alt=""></p>
<p>不同种类的神奇宝贝用的参数不同，用颜色区分。此时model在训练集上可以做的更好，在测试集上的结果也是比之前的18.1更好。</p>
<h2 id="还有其他因素的影响吗？"><a href="#还有其他因素的影响吗？" class="headerlink" title="还有其他因素的影响吗？"></a>还有其他因素的影响吗？</h2><p>比如对身高，体重，生命值进行绘图：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0mspg7tjj213k0sgdkf.jpg" alt=""></p>
<p>重新设计model：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mt6esluj213c0m6788.jpg" alt=""></p>
<p>考虑上生命值（$x<em>{hp}$）、高度（$x</em>{h}$）、重量（$x_{w}$）</p>
<p>这么复杂的model，理论上训练集上可以得到更好的结果，实际为1.9，确实是更低。但是测试集的结果就过拟合了。</p>
<h2 id="Regularization（正则化）"><a href="#Regularization（正则化）" class="headerlink" title="Regularization（正则化）"></a>Regularization（正则化）</h2><p>对于上面那么多参数结果并不理想的情况，这里进行正则化处理，将之前的损失函数进行修改：</p>
<p>$$<br>y = b + \sum w<em>{i}x</em>{i}  \qquad  (1-4)<br>$$</p>
<p>$$<br>L(f) = L(w, b) =\sum<em>{n=1}^{10}\left(\hat{y}^{n} -(b + w\cdot x</em>{cp}^{n})\right)^{2} + \lambda \sum (w_{i})^{2} \qquad   (1-5)<br>$$</p>
<p>式1-5 中多加了一项： $\lambda \sum (w<em>{i})^{2}$ ，结论是$w</em>{i}$越小，则方程（式1-4）就越好。还可以说当 $w_{i}$ 越小，则方程越平滑。</p>
<p>平滑的意思是当输入变化时，输出对输入的变化不敏感。比如式1-5 中输入增加了 $\Delta x<em>{i}$ 则输入就增加了 $w</em>{i}\Delta x<em>{i}$ ，可以看出当$w</em>{i}$越小，输出变化越不明显。还比如测试集的输入有一些噪音数据，越平滑的方程就会受到更小的影响。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0mtp38ryj212g0smte8.jpg" alt=""></p>
<p>上图是对 $\lambda$进行调整得出的结果。当 $\lambda$ 越大的时候， $\lambda \sum (w_{i})^{2}$ 这一项的影响力越大，所以当$\lambda$  越大的时候，方程越平滑。</p>
<p>训练集上得到的结果是：当 $\lambda$ 越大的时候，在训练集上得到的Error 是越大的。这是合理的现象，因为当 $\lambda$ 越大的时候，就越倾向于考虑 $w$ 本身值，减少考虑error。但是测试集上得到的error 是先减小又增大的。这里喜欢比较平滑的function，因为上面讲到对于噪音数据有很好的鲁棒性，所以开始增加 $\lambda$ 的时候性能是越来越好；但是又不喜欢太平滑的function，最平滑的function就是一条水平线了，那就相当于什么都没有做，所以太平滑的function又会得到糟糕的结果。</p>
<p> 所以最后这件事情就是找到最合适的 $\lambda$ ，此时带进式1-5 求出$b$ 和 $w_{i}$，得到的function就是最优的function。</p>
<blockquote>
<p>对于Regularization 的时候，多加的一项：$\lambda \sum (w_{i})^{2}$，并没有考虑 $b$ ，是因为期望得到平滑的function，但bias这项并不影响平滑程度，它只是将function上下移动，跟function的平滑程度是没有关系的。</p>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li><strong>Pokemon</strong>：原始的CP值极大程度的决定了进化后的CP值，但可能还有其他的一些因素。</li>
<li><strong>Gradient descent</strong>：梯度下降的做法；后面会讲到它的理论依据和要点。</li>
<li><strong>Overfitting</strong>和<strong>Regularization</strong>：过拟合和正则化，主要介绍了表象；后面会讲到更多这方面的理论</li>
</ul>
<blockquote>
<p>新博客地址：<a href="http://yoferzhang.com/post/20170326ML02Regression" target="_blank" rel="external">http://yoferzhang.com/post/20170326ML02Regression</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://melonteam.com/2017/03/27/ML02Regression/" data-id="cj0ur8cbr0005so1ugrhshbyx" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/27/ML03BiasAndVariance/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习入门系列03，Error的来源：偏差和方差（bias 和 variance）
        
      </div>
    </a>
  
  
    <a href="/2017/03/27/ML01Introduction/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习入门系列01，Introduction 简介</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IM/">IM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IOS/">IOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/我说/">我说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/视频/">视频</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_2/">深度学习入门实战（二）</a>
          </li>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_1/">深度学习入门实战（一）</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Video_codec_learning_sharing/">视频编解码学习分享</a>
          </li>
        
          <li>
            <a href="/2017/03/27/IOS_touch_event_distribution_mechanism/">IOS触摸事件分发机制详解</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Android_short_video_side_of_the_next_sowing_solution/">Android短视频边下边播详解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Melon Team<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>