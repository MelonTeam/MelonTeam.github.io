<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习入门系列04，Gradient Descent（梯度下降法） | Melon Team</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66632699

什么是Gradient">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习入门系列04，Gradient Descent（梯度下降法）">
<meta property="og:url" content="http://melonteam.com/2017/03/27/ML04GradientDescent/index.html">
<meta property="og:site_name" content="Melon Team">
<meta property="og:description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66632699

什么是Gradient">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0p6ppb7dj20z20ck76o.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p768yjij212g0msgps.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8iqqphj212a0n241f.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8x3ggkj20x00hsac3.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9awbcij21080ikmzo.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9pq9l1j210a0kmmzx.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pa69l64j212s0ng42l.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pakwyxsj213i0kik9e.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pb38dvnj212e0lu4d7.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pbh8oqtj21200s00xi.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pc5kle1j212g0nq48s.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pcmc0lcj20zk0iuq8m.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pd1h0xtj213c0m00x5.jpg">
<meta property="og:image" content="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdgqdzwj212q0kqn0q.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdwqnqzj212a0mingb.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0perzpcgj212g0t0qho.jpg">
<meta property="og:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pf85k6hj212s0jsdic.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pfmfsbvj21240j4amj.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pg12er4j212e0nyk53.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pghquu2j211y0n4wid.jpg">
<meta property="og:image" content="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pgz0bohj213e0nswjd.jpg">
<meta property="og:image" content="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0phhfnifj21200s0qet.jpg">
<meta property="og:updated_time" content="2017-03-28T07:44:43.626Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习入门系列04，Gradient Descent（梯度下降法）">
<meta name="twitter:description" content="引用课程：http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html

先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）

CSDN博客文章地址：http://blog.csdn.net/zyq522376829/article/details/66632699

什么是Gradient">
<meta name="twitter:image" content="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0p6ppb7dj20z20ck76o.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Melon Team" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Melon Team</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://melonteam.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-ML04GradientDescent" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/03/27/ML04GradientDescent/" class="article-date">
  <time datetime="2017-03-27T02:04:14.000Z" itemprop="datePublished">2017-03-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习入门系列04，Gradient Descent（梯度下降法）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>引用课程：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html" target="_blank" rel="external">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a></p>
</blockquote>
<p>先看这里，可能由于你正在查看这个平台行间公式不支持很多的渲染，所以最好在我的CSDN上查看，传送门：（无奈脸）</p>
<blockquote>
<p>CSDN博客文章地址：<a href="http://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="external">http://blog.csdn.net/zyq522376829/article/details/66632699</a></p>
</blockquote>
<h1 id="什么是Gradient-Descent（梯度下降法）？"><a href="#什么是Gradient-Descent（梯度下降法）？" class="headerlink" title="什么是Gradient Descent（梯度下降法）？"></a>什么是Gradient Descent（梯度下降法）？</h1><p>在第二篇文章中有介绍到梯度下降法的做法，传送门：<a href="地址地址地址">机器学习入门系列02，Regression 回归：案例研究</a></p>
<h2 id="Review-梯度下降法"><a href="#Review-梯度下降法" class="headerlink" title="Review: 梯度下降法"></a>Review: 梯度下降法</h2><a id="more"></a>
<p>在回归问题的第三步中，需要解决下面的最优化问题：</p>
<p>$$<br>\theta^{*} = \arg \min_{\theta} L(\theta)<br>$$</p>
<p>$$<br>L: loss function （损失函数）<br>$$</p>
<p>$$<br>\theta: parameters （参数）<br>$$</p>
<blockquote>
<p>这里的parameters是复数，即 $\theta$ 指代一堆参数，比如上篇说到的 $w$ 和 $b$。</p>
</blockquote>
<p>我们要找一组参数 $\theta$ ，让损失函数越小越好，这个问题可以用梯度下降法解决：</p>
<p>假设 $\theta$ 有里面有两个参数 ${\theta<em>{1}, \theta</em>{2}}$</p>
<p>随机选取初始值 </p>
<p>$$<br>\theta^{0} = \left[ \begin{matrix} \theta^{0}<em>{1}\ \theta^{0}</em>{2} \end{matrix} \right]<br>$$</p>
<p>这里可能某个平台不支持矩阵输入，看下图就好。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0p6ppb7dj20z20ck76o.jpg" alt=""></p>
<p>然后分别计算初始点处，两个参数对 $L$ 的偏微分，然后 $\theta^{0}$ 减掉 $\eta$ 乘上偏微分的值，得到一组新的参数。同理反复进行这样的计算。黄色部分为简洁的写法，$\nabla L(\theta)$即为<strong>梯度</strong>。</p>
<blockquote>
<p>$\eta$叫做Learning rates（学习速率）</p>
</blockquote>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p768yjij212g0msgps.jpg" alt=""></p>
<p>上图举例将梯度下降法的计算过程进行可视化。</p>
<h1 id="Tip1：调整-learning-rates（学习速率）"><a href="#Tip1：调整-learning-rates（学习速率）" class="headerlink" title="Tip1：调整 learning rates（学习速率）"></a>Tip1：调整 learning rates（学习速率）</h1><h2 id="小心翼翼地调整-learning-rate"><a href="#小心翼翼地调整-learning-rate" class="headerlink" title="小心翼翼地调整 learning rate"></a>小心翼翼地调整 learning rate</h2><p>举例：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p7olqw1j212k0mqjvn.jpg" alt=""></p>
<p>上图左边黑色为损失函数的曲线，假设从左边最高点开始，如果 learning rate 调整的刚刚好，比如红色的线，就能顺利找到最低点。如果  learning rate 调整的太小，比如蓝色的线，就会走的太慢，虽然这种情况给足够多的时间也可以找到最低点，实际情况可能会等不及出结果。如果  learning rate 调整的有点大，比如绿色的线，就会在上面震荡，走不下去，永远无法到达最低点。还有可能非常大，比如黄色的线，直接就飞出去了，update参数的时候只会发现损失函数越更新越大。</p>
<p>虽然这样的可视化可以很直观观察，但可视化也只是能在参数是一维或者二维的时候进行，更高维的情况已经无法可视化了。</p>
<p>解决方法就是上图右边的方案，将参数改变对损失函数的影响进行可视化。比如 learning rate 太小（蓝色的线），损失函数下降的非常慢；learning rate 太大（绿色的线），损失函数下降很快，但马上就卡住不下降了；learning rate特别大（黄色的线），损失函数就飞出去了；红色的就是差不多刚好，可以得到一个好的结果。</p>
<h2 id="自适应-learning-rate"><a href="#自适应-learning-rate" class="headerlink" title="自适应 learning rate"></a>自适应 learning rate</h2><p>举一个简单的思想：随着次数的增加，通过一些因子来减少 learning rate</p>
<ul>
<li>通常刚开始，初始点会距离最低点比较远，所以使用大一点的 learning rate</li>
<li>update好几次参数之后呢，比较靠近最低点了，此时减少 learning rate</li>
<li>比如 $\eta^{t} = \eta / \sqrt{t+1}$，$t$ 是次数。随着次数的增加，$\eta^{t}$ 减小</li>
</ul>
<p>但 learning rate 不能是 one-size-fits-all ，不同的参数需要不同的 learning rate</p>
<h2 id="Adagrad-算法"><a href="#Adagrad-算法" class="headerlink" title="Adagrad 算法"></a>Adagrad 算法</h2><h3 id="Adagrad-是什么？"><a href="#Adagrad-是什么？" class="headerlink" title="Adagrad 是什么？"></a>Adagrad 是什么？</h3><p>每个参数的学习率都把它除上之前微分的均方根。解释：</p>
<p>普通的梯度下降为：</p>
<p>$$<br>w^{t + 1} \leftarrow w^{t} - \eta^{t} g^{t}<br>$$</p>
<p>$$<br>\eta^{t} = \frac{\eta}{\sqrt{t+1}}<br>$$</p>
<p>$w$ 是一个参数</p>
<p>Adagrad 可以做的更好：</p>
<p>$$<br>w^{t+1} \leftarrow w^{t} - \frac{\eta^{t}}{\sigma} g^{t}<br>$$</p>
<p>$$<br>g^{t} = \frac{\partial L(\theta^{t})}{\partial w}<br>$$</p>
<p>$\sigma^{t}$:之前参数的所有微分的均方根，对于每个参数都是不一样的。</p>
<h3 id="Adagrad举例"><a href="#Adagrad举例" class="headerlink" title="Adagrad举例"></a>Adagrad举例</h3><p>下图是一个参数的更新过程</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8iqqphj212a0n241f.jpg" alt=""></p>
<p>将 Adagrad 的式子进行化简：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0p8x3ggkj20x00hsac3.jpg" alt=""></p>
<h3 id="Adagrad-存在的矛盾？"><a href="#Adagrad-存在的矛盾？" class="headerlink" title="Adagrad 存在的矛盾？"></a>Adagrad 存在的矛盾？</h3><p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9awbcij21080ikmzo.jpg" alt=""></p>
<p>在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。</p>
<p>下图是一个直观的解释：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0p9pq9l1j210a0kmmzx.jpg" alt=""></p>
<p>下面给一个正式的解释：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pa69l64j212s0ng42l.jpg" alt=""></p>
<p>比如初始点在 $x<em>{0}$，最低点为 $-\frac{b}{2a}$，最佳的步伐就是 $x</em>{0}$ 到最低点之间的距离 $| x<em>{0} + \frac{b}{2a} |$，也可以写成 $\frac{| 2ax</em>{0} + b|}{2a}$。而刚好 $ |2ax<em>{0} + b|$ 就是方程绝对值在$x</em>{0}$这一点的微分。</p>
<p>这样可以认为如果算出来的微分越大，则距离最低点越远。而且最好的步伐和微分的大小成正比。所以如果踏出去的步伐和微分成正比，它可能是比较好的。</p>
<p>结论1-1：梯度越大，就跟最低点的距离越远。</p>
<p>这个结论在多个参数的时候就不一定成立了。</p>
<h3 id="多参数下结论不一定成立"><a href="#多参数下结论不一定成立" class="headerlink" title="多参数下结论不一定成立"></a>多参数下结论不一定成立</h3><p><strong>对比不同的参数</strong></p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pakwyxsj213i0kik9e.jpg" alt=""></p>
<p>上图左边是两个参数的损失函数，颜色代表损失函数的值。如果只考虑参数 $w<em>{1}$，就像图中蓝色的线，得到右边上图结果；如果只考虑参数 $w</em>{2}$，就像图中绿色的线，得到右边下图的结果。确实对于a和b，结论1-1是成立的，同理c和b也成立。但是如果对比a和c，就不成立了，c比a大，但c距离最低点是比较近的。</p>
<p>所以结论1-1是在没有考虑跨参数对比的情况下，才能成立的。所以还不完善。</p>
<p>之前说到的最佳距离$\frac{| 2ax_{0} + b|}{2a}$，还有个分母 $2a$ 。对function进行二次微分刚好可以得到：</p>
<p>$$<br>\frac{\partial^{2}y}{\partial x^{2}} = 2a<br>$$</p>
<p>所以最好的步伐应该是：</p>
<p>$$<br>\frac{一次微分}{二次微分}<br>$$</p>
<p>即不止和一次微分成正比，还和二次微分成反比。最好的step应该考虑到二次微分：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pb38dvnj212e0lu4d7.jpg" alt=""></p>
<h3 id="Adagrad-进一步的解释"><a href="#Adagrad-进一步的解释" class="headerlink" title="Adagrad 进一步的解释"></a>Adagrad 进一步的解释</h3><p>再回到之前的 Adagrad</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pbh8oqtj21200s00xi.jpg" alt=""></p>
<p>对于$\sqrt{\sum^{t}_{i=0} (g^{i})^{2} }$ 就是希望再尽可能不增加过多运算的情况下模拟二次微分。（如果计算二次微分，在实际情况中可能会增加很多的时间消耗）</p>
<h1 id="Tip2：Stochastic-Gradient-Descent（随机梯度下降法）"><a href="#Tip2：Stochastic-Gradient-Descent（随机梯度下降法）" class="headerlink" title="Tip2：Stochastic Gradient Descent（随机梯度下降法）"></a>Tip2：Stochastic Gradient Descent（随机梯度下降法）</h1><p>之前的梯度下降：</p>
<p>$$<br>L =\sum<em>{n} \left( \hat{y}^{n} - (b + \sum w</em>{i} x^{n}_{i})   \right)^{2}<br>$$</p>
<p>$$<br>\theta^{i} = \theta^{i -1} - \eta \nabla L(\theta^{i -1})<br>$$</p>
<p>而Stochastic Gradient Descent（更快）：</p>
<p>损失函数不需要处理训练集所有的数据，选取一个例子 $x^{n}$</p>
<p>$$<br>L^{n} = \left( \hat{y}^{n} - (b + \sum w<em>{i} x^{n}</em>{i}   \right)^{2}<br>$$</p>
<p>$$<br>\theta^{i} = \theta^{i -1} - \eta \nabla L^{n}(\theta^{i -1})<br>$$</p>
<p>此时不需要像之前那样对所有的数据进行处理，只需要计算某一个例子的损失函数$L^{n}$，就可以赶紧update 梯度。</p>
<p>对比：</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pc5kle1j212g0nq48s.jpg" alt=""></p>
<p>常规梯度下降法走一步要处理到所有二十个examples，但Stochastic 此时已经走了二十步（没处理一个example就更新）</p>
<h1 id="Tip3：Feature-Scaling（特征缩放）"><a href="#Tip3：Feature-Scaling（特征缩放）" class="headerlink" title="Tip3：Feature Scaling（特征缩放）"></a>Tip3：Feature Scaling（特征缩放）</h1><p>比如有个function：</p>
<p>$$<br>y = b + w<em>{1}x</em>{1} + w<em>{2}x</em>{2}<br>$$</p>
<p>两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pcmc0lcj20zk0iuq8m.jpg" alt=""></p>
<h2 id="为什么要这样做？"><a href="#为什么要这样做？" class="headerlink" title="为什么要这样做？"></a>为什么要这样做？</h2><p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pd1h0xtj213c0m00x5.jpg" alt=""></p>
<p>上图左边是$x<em>{1}$的scale比 $x</em>{2}$要小很多，所以当$w<em>{1}$ 和 $w</em>{2}$做同样的变化时，$w<em>{1}$对y的变化影响是比较小的，$x</em>{2}$对y的变化影响是比较大的。</p>
<p>坐标系中是两个参数的error surface（现在考虑左边蓝色），因为$w<em>{1}$对y的变化影响比较小，所以$w</em>{1}$对损失函数的影响比较小，$w<em>{1}$对损失函数有比较小的微分，所以$w</em>{1}$方向上是比较平滑的。同理$x<em>{2}$对y的影响比较大，所以$x</em>{2}$对损失函数的影响比较大，所以在$x_{2}$方向有比较尖的峡谷。</p>
<p>上图右边是两个参数scaling比较接近，右边的绿色图就比较接近圆形。</p>
<p>对于左边的情况，上面讲过这种狭长的情形不过不用Adagrad的话是比较难处理的，两个方向上需要不同的学习率，同一组学习率会搞不定它。而右边情形更新参数就会变得比较容易。左边的梯度下降并不是向着最低点方向走的，而是顺着等高线切线法线方向走的。但绿色就可以向着圆心（最低点）走，这样做参数更新也是比较有效率。</p>
<h2 id="怎么做-scaling？"><a href="#怎么做-scaling？" class="headerlink" title="怎么做 scaling？"></a>怎么做 scaling？</h2><p>方法非常多，这里举例一种常见的做法：</p>
<p><img src="http://wx1.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdgqdzwj212q0kqn0q.jpg" alt=""></p>
<p>上图每一列都是一个例子，里面都有一组feature。</p>
<p>对每一个维度$i$（绿色框）都计算平均数，记做$m<em>{i}$；还要计算标准差，记做$\sigma</em>{i}$。</p>
<p>然后用第r个例子中的第i个输入，减掉平均数$m<em>{i}$，然后除以标准差$\sigma</em>{i}$，得到的结果是所有的维数都是0，所有的方差都是1</p>
<h1 id="梯度下降的理论基础"><a href="#梯度下降的理论基础" class="headerlink" title="梯度下降的理论基础"></a>梯度下降的理论基础</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>当用梯度下降解决问题：</p>
<p>$$<br>\theta^{*} = \arg \min_{\theta} L(\theta)<br>$$</p>
<p>每次更新参数 $\theta$，都得到一个新的 $\theta$，它都使得损失函数更小。即：</p>
<p>$$<br>L(\theta^{0}) &gt; L(\theta^{1}) &gt; L(\theta^{2})&gt;\cdots<br>$$</p>
<p>上述结论正确吗？</p>
<p>结论是不正确的。。。</p>
<h1 id="数学理论"><a href="#数学理论" class="headerlink" title="数学理论"></a>数学理论</h1><p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pdwqnqzj212a0mingb.jpg" alt=""></p>
<p>比如在$\theta^{0}$处，可以在一个小范围的圆圈内找到损失函数细小的$\theta^{1}$，不断的这样去寻找。</p>
<p>接下来就是如果在小圆圈内快速的找到最小值？</p>
<h2 id="Taylor-Series（泰勒展开式）"><a href="#Taylor-Series（泰勒展开式）" class="headerlink" title="Taylor Series（泰勒展开式）"></a>Taylor Series（泰勒展开式）</h2><p>先介绍一下泰勒展开式</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>若$h(x)$在$x = x_{0}$点的某个领域内有无限阶导数（即无限可微分，infinitely differentiable），那么在此领域内有：</p>
<p>$$<br>h(x) = \sum^{\infty}<em>{k = 0} \frac{h^{k}(x</em>{0})}{k!} (x - x_{0})^{k}<br>$$</p>
<p>$$<br>=h(x<em>{0}) + h’(x</em>{0})(x - x<em>{0}) + \frac{h’’(x</em>{0})}{2!}(x - x_{0})^2 + \cdots  \qquad  (1-1)<br>$$</p>
<p>当$x$很接近$x<em>{0}$时，有$h(x) \approx h(x</em>{0}) + h’(x<em>{0})(x - x</em>{0})$</p>
<p>式1-1就是函数$h(x)$在$x = x_{0}$点附近关于$x$的幂函数展开式，也叫<strong>泰勒展开式</strong>。</p>
<p>举例：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0perzpcgj212g0t0qho.jpg" alt=""></p>
<p>图中3条蓝色线是把前3项作图，橙色线是 $sin(x)$。</p>
<h3 id="多变量泰勒展开式"><a href="#多变量泰勒展开式" class="headerlink" title="多变量泰勒展开式"></a>多变量泰勒展开式</h3><p>下面是两个变量的泰勒展开式</p>
<p><img src="http://wx2.sinaimg.cn/mw690/a9c4d5f6gy1fe0pf85k6hj212s0jsdic.jpg" alt=""></p>
<h2 id="利用泰勒展开式简化"><a href="#利用泰勒展开式简化" class="headerlink" title="利用泰勒展开式简化"></a>利用泰勒展开式简化</h2><p>回到之前如何快速在圆圈内找到最小值。基于泰勒展开式，在$(a,b)$ 点的红色圆圈范围内，可以将损失函数用泰勒展开式进行简化：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pfmfsbvj21240j4amj.jpg" alt=""></p>
<p>将问题进而简化为下图：</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pg12er4j212e0nyk53.jpg" alt=""></p>
<p>不考虑s的话，可以看出剩下的部分就是两个向量$(\Delta \theta<em>{1}, \Delta \theta</em>{2})$ 和 $(u, v)$的内积，那怎样让它最小，就是和向量 $(u, v)$ 方向相反的向量</p>
<p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0pghquu2j211y0n4wid.jpg" alt=""></p>
<p>然后将u和v带入。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/a9c4d5f6gy1fe0pgz0bohj213e0nswjd.jpg" alt=""></p>
<p>$$<br>L(\theta) \approx s + u(\theta<em>{1} - a) + v(\theta</em>{2} - b)    (1-2)<br>$$</p>
<p>发现最后的式子就是梯度下降的式子。但这里用这种方法找到这个式子有个前提，泰勒展开式给的损失函数的估算值是要足够精确的，而这需要红色的圈圈足够小（也就是学习率足够小）来保证。所以理论上每次更新参数都想要损失函数减小的话，即保证式1-2 成立的话，就需要学习率足够足够小才可以。</p>
<p>所以实际中，当更新参数的时候，如果学习率没有设好，是有可能式1-2是不成立的，所以导致做梯度下降的时候，损失函数没有越来越小。</p>
<blockquote>
<p>式1-2只考虑了泰勒展开式的一次项，如果考虑到二次项（比如牛顿法），在实际中不是特别好，会涉及到二次微分等，多很多的运算，性价比不好。</p>
</blockquote>
<h1 id="梯度下降的限制"><a href="#梯度下降的限制" class="headerlink" title="梯度下降的限制"></a>梯度下降的限制</h1><p><img src="http://wx4.sinaimg.cn/mw690/a9c4d5f6gy1fe0phhfnifj21200s0qet.jpg" alt=""></p>
<ul>
<li>容易陷入局部极值</li>
<li>还有可能卡在不是极值，但微分值是0的地方</li>
<li>还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点</li>
</ul>
<blockquote>
<p>新博客地址：<a href="http://yoferzhang.com/post/20170327ML04GradientDescent" target="_blank" rel="external">http://yoferzhang.com/post/20170327ML04GradientDescent</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://melonteam.com/2017/03/27/ML04GradientDescent/" data-id="cj0uqj9200006us1ubudrh5qu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/27/Android_short_video_side_of_the_next_sowing_solution/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Android短视频边下边播详解
        
      </div>
    </a>
  
  
    <a href="/2017/03/27/ML03BiasAndVariance/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习入门系列03，Error的来源：偏差和方差（bias 和 variance）</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IM/">IM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/IOS/">IOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/我说/">我说</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/视频/">视频</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_2/">深度学习入门实战（二）</a>
          </li>
        
          <li>
            <a href="/2017/03/28/Deep_learning_entry_1/">深度学习入门实战（一）</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Video_codec_learning_sharing/">视频编解码学习分享</a>
          </li>
        
          <li>
            <a href="/2017/03/27/IOS_touch_event_distribution_mechanism/">IOS触摸事件分发机制详解</a>
          </li>
        
          <li>
            <a href="/2017/03/27/Android_short_video_side_of_the_next_sowing_solution/">Android短视频边下边播详解</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Melon Team<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>